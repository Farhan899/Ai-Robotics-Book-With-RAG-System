"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[820],{1802(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>i,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"capstone/code-examples","title":"Capstone Code Examples: Complete Integration Code","description":"Overview","source":"@site/docs/capstone/code-examples.md","sourceDirName":"capstone","slug":"/capstone/code-examples","permalink":"/docs/capstone/code-examples","draft":false,"unlisted":false,"editUrl":"https://github.com/Farhan899/Ai-Robotics-Book/edit/main/docs/capstone/code-examples.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Capstone Hands-On: Building the Autonomous Humanoid","permalink":"/docs/capstone/hands-on"},"next":{"title":"Capstone Troubleshooting: Complex Integration Troubleshooting","permalink":"/docs/capstone/troubleshooting"}}');var a=t(4848),s=t(8453);const i={sidebar_position:4},r="Capstone Code Examples: Complete Integration Code",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Complete Voice Command Integration Example",id:"complete-voice-command-integration-example",level:2},{value:"Voice Command Processing Node",id:"voice-command-processing-node",level:3},{value:"Complete Perception System Example",id:"complete-perception-system-example",level:2},{value:"Advanced Perception Node with Isaac ROS Integration",id:"advanced-perception-node-with-isaac-ros-integration",level:3},{value:"Complete Navigation System Example",id:"complete-navigation-system-example",level:2},{value:"Navigation Integration with Path Planning",id:"navigation-integration-with-path-planning",level:3},{value:"Complete Manipulation System Example",id:"complete-manipulation-system-example",level:2},{value:"Manipulation Node with Grasp Planning",id:"manipulation-node-with-grasp-planning",level:3},{value:"Complete System Integration Example",id:"complete-system-integration-example",level:2},{value:"Main Integration Node",id:"main-integration-node",level:3},{value:"Launch Files and Configuration",id:"launch-files-and-configuration",level:2},{value:"Main Launch File for Complete System",id:"main-launch-file-for-complete-system",level:3},{value:"Testing and Validation Scripts",id:"testing-and-validation-scripts",level:2},{value:"System Test Script",id:"system-test-script",level:3},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Practical Exercises",id:"practical-exercises",level:2},{value:"Common Failure Modes",id:"common-failure-modes",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"capstone-code-examples-complete-integration-code",children:"Capstone Code Examples: Complete Integration Code"})}),"\n",(0,a.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(n.p,{children:"This module provides complete, working code examples for the autonomous humanoid system. These examples demonstrate how to integrate all components from the previous modules into a functioning voice-controlled robot system. Each example includes detailed explanations and can be used as a foundation for your own implementations."}),"\n",(0,a.jsx)(n.h2,{id:"complete-voice-command-integration-example",children:"Complete Voice Command Integration Example"}),"\n",(0,a.jsx)(n.h3,{id:"voice-command-processing-node",children:"Voice Command Processing Node"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n# File: humanoid_voice_control/humanoid_voice_control/voice_command_node.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom audio_common_msgs.msg import AudioData\nimport speech_recognition as sr\nimport openai\nimport json\nimport threading\nimport queue\nfrom humanoid_msgs.msg import ParsedCommand\nfrom humanoid_msgs.srv import CommandValidation\nimport os\n\nclass VoiceCommandNode(Node):\n    def __init__(self):\n        super().__init__('voice_command_node')\n\n        # Initialize speech recognizer\n        self.recognizer = sr.Recognizer()\n        self.recognizer.energy_threshold = 4000\n        self.recognizer.dynamic_energy_threshold = True\n\n        # Audio queue for processing\n        self.audio_queue = queue.Queue()\n\n        # Publishers and subscribers\n        self.voice_cmd_pub = self.create_publisher(String, '/voice_commands', 10)\n        self.parsed_cmd_pub = self.create_publisher(ParsedCommand, '/parsed_commands', 10)\n        self.audio_sub = self.create_subscription(\n            AudioData, '/audio', self.audio_callback, 10)\n\n        # Service for command validation\n        self.validation_srv = self.create_service(\n            CommandValidation, '/validate_command', self.validate_command_callback)\n\n        # Timer for audio processing\n        self.process_timer = self.create_timer(0.1, self.process_audio)\n\n        # Initialize OpenAI with API key from environment\n        openai.api_key = os.getenv('OPENAI_API_KEY', 'your-openai-key')\n\n        self.get_logger().info('Voice Command Node initialized')\n\n    def audio_callback(self, msg):\n        \"\"\"Callback for audio data\"\"\"\n        self.audio_queue.put(msg)\n\n    def process_audio(self):\n        \"\"\"Process audio from queue\"\"\"\n        try:\n            audio_data = self.audio_queue.get_nowait()\n            self.process_audio_data(audio_data)\n        except queue.Empty:\n            pass\n\n    def process_audio_data(self, audio_data):\n        \"\"\"Process raw audio data to text\"\"\"\n        try:\n            # Convert audio data to audio file format\n            with sr.AudioData(audio_data.data, 16000, 2) as source:\n                # Perform speech recognition\n                text = self.recognizer.recognize_google(source)\n\n                if text:\n                    # Publish the recognized text\n                    cmd_msg = String()\n                    cmd_msg.data = text\n                    self.voice_cmd_pub.publish(cmd_msg)\n\n                    # Log the recognized command\n                    self.get_logger().info(f'Recognized command: {text}')\n\n                    # Parse and publish structured command\n                    parsed_cmd = self.parse_command_with_llm(text)\n                    if parsed_cmd:\n                        self.parsed_cmd_pub.publish(parsed_cmd)\n\n        except sr.UnknownValueError:\n            self.get_logger().warn('Could not understand audio')\n        except sr.RequestError as e:\n            self.get_logger().error(f'Could not request results from speech service; {e}')\n        except Exception as e:\n            self.get_logger().error(f'Error processing audio: {e}')\n\n    def parse_command_with_llm(self, text):\n        \"\"\"Use LLM to parse natural language command into structured format\"\"\"\n        try:\n            # Define the system prompt for command parsing\n            system_prompt = \"\"\"You are a command parser for a humanoid robot.\n            Parse the user's natural language command into a structured format.\n            Return JSON with action_type, target_object, target_location, and parameters.\n            Action types: 'navigation', 'manipulation', 'combined', 'query', 'other'.\n            Example: {'action_type': 'navigation', 'target_location': 'kitchen',\n            'target_object': null, 'parameters': {}}\"\"\"\n\n            response = openai.ChatCompletion.create(\n                model=\"gpt-3.5-turbo\",\n                messages=[\n                    {\"role\": \"system\", \"content\": system_prompt},\n                    {\"role\": \"user\", \"content\": text}\n                ],\n                temperature=0.1,\n                max_tokens=150\n            )\n\n            # Parse the response\n            result = json.loads(response.choices[0].message.content)\n\n            # Create parsed command message\n            parsed_cmd = ParsedCommand()\n            parsed_cmd.action_type = result.get('action_type', 'other')\n            parsed_cmd.target_object = result.get('target_object', '')\n            parsed_cmd.target_location = result.get('target_location', '')\n            parsed_cmd.parameters = json.dumps(result.get('parameters', {}))\n            parsed_cmd.original_command = text\n            parsed_cmd.confidence = 0.9  # Assuming high confidence for LLM output\n\n            self.get_logger().info(f'Parsed command: {parsed_cmd.action_type} to {parsed_cmd.target_location} for {parsed_cmd.target_object}')\n            return parsed_cmd\n\n        except json.JSONDecodeError:\n            self.get_logger().error(f'LLM response not valid JSON: {response.choices[0].message.content}')\n            return None\n        except Exception as e:\n            self.get_logger().error(f'Error parsing command with LLM: {e}')\n            return None\n\n    def validate_command_callback(self, request, response):\n        \"\"\"Validate if a command is safe to execute\"\"\"\n        # Define dangerous keywords that should not be executed\n        dangerous_keywords = [\n            'harm', 'damage', 'break', 'destroy', 'hurt', 'injure',\n            'attack', 'kill', 'unsafe', 'dangerous', 'emergency'\n        ]\n\n        # Check if command contains dangerous keywords\n        is_safe = not any(keyword in request.command.lower() for keyword in dangerous_keywords)\n\n        if is_safe:\n            response.is_valid = True\n            response.reason = \"Command is safe\"\n        else:\n            response.is_valid = False\n            response.reason = \"Command contains potentially unsafe keywords\"\n\n        self.get_logger().info(f'Command validation: {request.command} -> {response.is_valid}')\n        return response\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VoiceCommandNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info('Shutting down voice command node')\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"complete-perception-system-example",children:"Complete Perception System Example"}),"\n",(0,a.jsx)(n.h3,{id:"advanced-perception-node-with-isaac-ros-integration",children:"Advanced Perception Node with Isaac ROS Integration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n# File: humanoid_perception/humanoid_perception/advanced_perception_node.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo, PointCloud2\nfrom vision_msgs.msg import Detection2DArray, ObjectHypothesisWithPose\nfrom geometry_msgs.msg import Point, Pose, TransformStamped\nfrom tf2_ros import TransformBroadcaster\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nfrom humanoid_msgs.msg import DetectedObject, SemanticScene\nfrom builtin_interfaces.msg import Time\nimport tf2_ros\nimport tf2_geometry_msgs\nfrom geometry_msgs.msg import Vector3, Quaternion\nfrom visualization_msgs.msg import Marker, MarkerArray\n\nclass AdvancedPerceptionNode(Node):\n    def __init__(self):\n        super().__init__('advanced_perception_node')\n\n        # Initialize CV bridge\n        self.bridge = CvBridge()\n\n        # TF broadcaster for transforms\n        self.tf_broadcaster = TransformBroadcaster(self)\n\n        # Publishers and subscribers\n        self.image_sub = self.create_subscription(\n            Image, '/camera/image_raw', self.image_callback, 10)\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo, '/camera/camera_info', self.camera_info_callback, 10)\n        self.pointcloud_sub = self.create_subscription(\n            PointCloud2, '/camera/depth/points', self.pointcloud_callback, 10)\n\n        self.detection_pub = self.create_publisher(\n            Detection2DArray, '/detected_objects', 10)\n        self.semantic_pub = self.create_publisher(\n            SemanticScene, '/semantic_scene', 10)\n        self.marker_pub = self.create_publisher(\n            MarkerArray, '/object_markers', 10)\n\n        # Camera parameters\n        self.camera_matrix = None\n        self.dist_coeffs = None\n        self.image_width = 640\n        self.image_height = 480\n\n        # Object detection parameters\n        self.confidence_threshold = 0.5\n        self.min_object_area = 500\n\n        # Detected objects storage\n        self.detected_objects = []\n        self.object_colors = {}  # Track colors for object persistence\n\n        # Initialize YOLO model (simplified - in practice use Isaac ROS DNN)\n        # For this example, we'll use OpenCV's DNN module\n        try:\n            # Load YOLO model (you would download these files in a real implementation)\n            # self.net = cv2.dnn.readNetFromDarknet('yolo.cfg', 'yolo.weights')\n            # For this example, we'll use a simpler approach\n            self.get_logger().info('Perception node initialized with basic detection')\n        except Exception as e:\n            self.get_logger().warn(f'Could not load advanced detection model: {e}')\n            self.get_logger().info('Using basic color-based detection instead')\n\n        self.get_logger().info('Advanced Perception Node initialized')\n\n    def camera_info_callback(self, msg):\n        \"\"\"Callback for camera info\"\"\"\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\n        self.dist_coeffs = np.array(msg.d)\n        self.image_width = msg.width\n        self.image_height = msg.height\n\n    def pointcloud_callback(self, msg):\n        \"\"\"Callback for point cloud data\"\"\"\n        # Process point cloud data for 3D object information\n        # This would typically interface with Isaac ROS point cloud processing\n        pass\n\n    def image_callback(self, msg):\n        \"\"\"Callback for image data\"\"\"\n        try:\n            # Convert ROS image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n            # Perform object detection\n            detections = self.detect_objects(cv_image)\n\n            # Create and publish detections\n            detection_msg = self.create_detection_message(detections, msg.header)\n            self.detection_pub.publish(detection_msg)\n\n            # Create and publish semantic scene\n            semantic_msg = self.create_semantic_scene(detections, msg.header)\n            self.semantic_pub.publish(semantic_msg)\n\n            # Publish visualization markers\n            marker_array = self.create_visualization_markers(detections, msg.header)\n            self.marker_pub.publish(marker_array)\n\n            # Publish TF transforms for detected objects\n            self.publish_object_transforms(detections, msg.header)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n\n    def detect_objects(self, image):\n        \"\"\"Perform object detection on image\"\"\"\n        detections = []\n\n        # For this example, we'll implement a more sophisticated approach\n        # that combines color-based detection with basic shape analysis\n\n        # Convert to HSV for color detection\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n        # Define color ranges for different objects\n        color_ranges = {\n            'red': ([0, 100, 100], [10, 255, 255]),\n            'red2': ([170, 100, 100], [180, 255, 255]),  # Red wraps around HSV\n            'blue': ([100, 100, 100], [130, 255, 255]),\n            'green': ([40, 100, 100], [80, 255, 255]),\n            'yellow': ([20, 100, 100], [40, 255, 255])\n        }\n\n        for color_name, (lower, upper) in color_ranges.items():\n            lower = np.array(lower)\n            upper = np.array(upper)\n\n            # Create mask for this color\n            if color_name == 'red2':\n                mask = cv2.inRange(hsv, lower, upper)\n                mask = mask + cv2.inRange(hsv, np.array([0, 100, 100]), np.array([10, 255, 255]))\n            else:\n                mask = cv2.inRange(hsv, lower, upper)\n\n            # Apply morphological operations to clean up the mask\n            kernel = np.ones((5,5), np.uint8)\n            mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n            mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n\n            # Find contours\n            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n            for contour in contours:\n                area = cv2.contourArea(contour)\n                if area > self.min_object_area:  # Filter small areas\n                    # Get bounding box\n                    x, y, w, h = cv2.boundingRect(contour)\n\n                    # Calculate center\n                    center_x = x + w // 2\n                    center_y = y + h // 2\n\n                    # Determine object type based on shape and size\n                    aspect_ratio = float(w) / h\n                    extent = float(area) / (w * h)\n\n                    if aspect_ratio > 0.8 and aspect_ratio < 1.2:\n                        obj_type = f'{color_name}_object'\n                    elif aspect_ratio < 0.7:\n                        obj_type = f'{color_name}_tall_object'\n                    else:\n                        obj_type = f'{color_name}_wide_object'\n\n                    detection = {\n                        'class': obj_type,\n                        'confidence': min(0.9, area / 10000.0),  # Normalize confidence based on size\n                        'bbox': [x, y, w, h],\n                        'center': [center_x, center_y],\n                        'area': area\n                    }\n                    detections.append(detection)\n\n        return detections\n\n    def create_detection_message(self, detections, header):\n        \"\"\"Create vision_msgs/Detection2DArray message\"\"\"\n        detection_array = Detection2DArray()\n        detection_array.header = header\n\n        for detection in detections:\n            if detection['confidence'] > self.confidence_threshold:\n                vision_detection = Detection2D()\n\n                # Set bounding box\n                vision_detection.bbox.center.x = float(detection['center'][0])\n                vision_detection.bbox.center.y = float(detection['center'][1])\n                vision_detection.bbox.size_x = float(detection['bbox'][2])\n                vision_detection.bbox.size_y = float(detection['bbox'][3])\n\n                # Set results\n                result = ObjectHypothesisWithPose()\n                result.hypothesis.class_id = detection['class']\n                result.hypothesis.score = detection['confidence']\n                vision_detection.results.append(result)\n\n                detection_array.detections.append(vision_detection)\n\n        return detection_array\n\n    def create_semantic_scene(self, detections, header):\n        \"\"\"Create semantic scene message\"\"\"\n        semantic_scene = SemanticScene()\n        semantic_scene.header = header\n\n        for detection in detections:\n            if detection['confidence'] > self.confidence_threshold:\n                obj = DetectedObject()\n                obj.name = detection['class']\n                obj.confidence = detection['confidence']\n\n                # Convert 2D image coordinates to 3D world coordinates if camera matrix is available\n                if self.camera_matrix is not None and self.dist_coeffs is not None:\n                    # Simple depth assumption for example - in practice you'd use depth data\n                    # For this example, we'll assume all objects are at 1 meter depth\n                    image_point = np.array([[detection['center'][0], detection['center'][1]]], dtype=np.float32)\n                    world_point = cv2.undistortPoints(image_point, self.camera_matrix, self.dist_coeffs)\n\n                    # Convert to 3D assuming Z=1 (in front of camera)\n                    obj.position.x = world_point[0][0][0] * 1.0  # Scale by assumed depth\n                    obj.position.y = world_point[0][0][1] * 1.0\n                    obj.position.z = 1.0  # Assumed depth\n                else:\n                    # Fallback to 2D coordinates\n                    obj.position.x = detection['center'][0]\n                    obj.position.y = detection['center'][1]\n                    obj.position.z = 0.0\n\n                obj.dimensions.x = detection['bbox'][2]  # Width\n                obj.dimensions.y = detection['bbox'][3]  # Height\n                obj.dimensions.z = 0.1  # Assumed depth\n\n                semantic_scene.objects.append(obj)\n\n        return semantic_scene\n\n    def create_visualization_markers(self, detections, header):\n        \"\"\"Create visualization markers for detected objects\"\"\"\n        marker_array = MarkerArray()\n\n        for i, detection in enumerate(detections):\n            if detection['confidence'] > self.confidence_threshold:\n                # Create text marker for object label\n                text_marker = Marker()\n                text_marker.header = header\n                text_marker.ns = \"object_labels\"\n                text_marker.id = i * 2\n                text_marker.type = Marker.TEXT_VIEW_FACING\n                text_marker.action = Marker.ADD\n\n                # Position text above the object\n                if self.camera_matrix is not None:\n                    # Convert image coordinates to approximate world coordinates\n                    text_marker.pose.position.x = detection['center'][0] * 0.001  # Scale factor\n                    text_marker.pose.position.y = detection['center'][1] * 0.001\n                    text_marker.pose.position.z = 1.2  # Slightly above the object\n                else:\n                    text_marker.pose.position.x = detection['center'][0]\n                    text_marker.pose.position.y = detection['center'][1]\n                    text_marker.pose.position.z = 0.1\n\n                text_marker.pose.orientation.w = 1.0\n                text_marker.scale.z = 0.1  # Text size\n                text_marker.color.a = 1.0\n                text_marker.color.r = 1.0\n                text_marker.color.g = 1.0\n                text_marker.color.b = 1.0\n                text_marker.text = f\"{detection['class']}: {detection['confidence']:.2f}\"\n\n                marker_array.markers.append(text_marker)\n\n                # Create box marker for bounding box\n                box_marker = Marker()\n                box_marker.header = header\n                box_marker.ns = \"object_boxes\"\n                box_marker.id = i * 2 + 1\n                box_marker.type = Marker.LINE_STRIP\n                box_marker.action = Marker.ADD\n\n                # Define the 4 corners of the bounding box\n                x, y, w, h = detection['bbox']\n                corners = [\n                    Point(x=float(x), y=float(y), z=0.01),\n                    Point(x=float(x + w), y=float(y), z=0.01),\n                    Point(x=float(x + w), y=float(y + h), z=0.01),\n                    Point(x=float(x), y=float(y + h), z=0.01),\n                    Point(x=float(x), y=float(y), z=0.01)  # Close the loop\n                ]\n\n                box_marker.points = corners\n                box_marker.pose.orientation.w = 1.0\n                box_marker.scale.x = 0.02  # Line width\n                box_marker.color.a = 0.8\n                box_marker.color.r = 1.0\n                box_marker.color.g = 1.0\n                box_marker.color.b = 0.0\n\n                marker_array.markers.append(box_marker)\n\n        return marker_array\n\n    def publish_object_transforms(self, detections, header):\n        \"\"\"Publish TF transforms for detected objects\"\"\"\n        for i, detection in enumerate(detections):\n            if detection['confidence'] > self.confidence_threshold:\n                t = TransformStamped()\n\n                t.header.stamp = self.get_clock().now().to_msg()\n                t.header.frame_id = header.frame_id\n                t.child_frame_id = f\"detected_{detection['class']}_{i}\"\n\n                # Set transform position\n                if self.camera_matrix is not None:\n                    # Convert image coordinates to approximate world coordinates\n                    t.transform.translation.x = detection['center'][0] * 0.001\n                    t.transform.translation.y = detection['center'][1] * 0.001\n                    t.transform.translation.z = 1.0  # Assumed depth\n                else:\n                    t.transform.translation.x = detection['center'][0] * 0.001\n                    t.transform.translation.y = detection['center'][1] * 0.001\n                    t.transform.translation.z = 0.1\n\n                # No rotation for detected objects\n                t.transform.rotation.w = 1.0\n\n                # Publish the transform\n                self.tf_broadcaster.sendTransform(t)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = AdvancedPerceptionNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info('Shutting down advanced perception node')\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"complete-navigation-system-example",children:"Complete Navigation System Example"}),"\n",(0,a.jsx)(n.h3,{id:"navigation-integration-with-path-planning",children:"Navigation Integration with Path Planning"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# File: humanoid_navigation/humanoid_navigation/advanced_navigation_node.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped, Point, Pose, Quaternion\nfrom humanoid_msgs.msg import ParsedCommand, NavigationGoal, SemanticScene\nfrom humanoid_msgs.srv import NavigateToPose\nfrom nav2_msgs.action import NavigateToPose as NavigateToPoseAction\nfrom rclpy.action import ActionClient\nfrom tf2_ros import TransformListener, Buffer\nfrom builtin_interfaces.msg import Duration\nimport json\nimport math\n\nclass AdvancedNavigationNode(Node):\n    def __init__(self):\n        super().__init__(\'advanced_navigation_node\')\n\n        # Action client for Nav2\n        self.nav_client = ActionClient(self, NavigateToPoseAction, \'navigate_to_pose\')\n\n        # TF listener for transforms\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        # Publishers and subscribers\n        self.command_sub = self.create_subscription(\n            ParsedCommand, \'/parsed_commands\', self.command_callback, 10)\n        self.nav_goal_pub = self.create_publisher(\n            NavigationGoal, \'/navigation_goals\', 10)\n        self.semantic_scene_sub = self.create_subscription(\n            SemanticScene, \'/semantic_scene\', self.semantic_scene_callback, 10)\n\n        # Service for navigation\n        self.nav_srv = self.create_service(\n            NavigateToPose, \'/navigate_to_pose\', self.navigate_to_pose_callback)\n\n        # Known locations mapping with poses\n        self.location_map = {\n            \'kitchen\': Pose(position=Point(x=2.0, y=1.0, z=0.0),\n                           orientation=Quaternion(w=1.0)),\n            \'living_room\': Pose(position=Point(x=-1.0, y=0.0, z=0.0),\n                              orientation=Quaternion(w=1.0)),\n            \'bedroom\': Pose(position=Point(x=0.0, y=-2.0, z=0.0),\n                           orientation=Quaternion(w=1.0)),\n            \'office\': Pose(position=Point(x=1.5, y=-1.0, z=0.0),\n                         orientation=Quaternion(w=1.0)),\n            \'entrance\': Pose(position=Point(x=0.0, y=2.0, z=0.0),\n                           orientation=Quaternion(w=1.0)),\n            \'charging_station\': Pose(position=Point(x=-2.0, y=0.0, z=0.0),\n                                   orientation=Quaternion(w=1.0))\n        }\n\n        # Semantic scene storage\n        self.semantic_scene = None\n        self.last_navigation_time = self.get_clock().now()\n\n        self.get_logger().info(\'Advanced Navigation Node initialized\')\n\n    def semantic_scene_callback(self, msg):\n        """Callback for semantic scene updates"""\n        self.semantic_scene = msg\n        self.get_logger().info(f\'Updated semantic scene with {len(msg.objects)} objects\')\n\n    def command_callback(self, msg):\n        """Callback for parsed commands"""\n        if msg.action_type in [\'navigation\', \'combined\']:\n            self.handle_navigation_command(msg)\n\n    def handle_navigation_command(self, cmd_msg):\n        """Handle navigation commands"""\n        target_location = cmd_msg.target_location.lower()\n\n        # Try to find the location in our map\n        if target_location in self.location_map:\n            target_pose = self.location_map[target_location]\n            self.get_logger().info(f\'Navigating to known location: {target_location}\')\n        else:\n            # Try to find the location as an object in the scene\n            target_pose = self.find_object_in_scene(target_location)\n            if target_pose is None:\n                self.get_logger().warn(f\'Unknown location/object: {target_location}\')\n                return\n\n        # Create navigation goal\n        goal_msg = NavigateToPoseAction.Goal()\n        goal_msg.pose.header.frame_id = \'map\'\n        goal_msg.pose.header.stamp = self.get_clock().now().to_msg()\n        goal_msg.pose.pose = target_pose\n\n        # Send navigation goal\n        self.send_navigation_goal(goal_msg)\n\n        # Publish navigation goal message\n        nav_goal_msg = NavigationGoal()\n        nav_goal_msg.target_location = target_location\n        nav_goal_msg.target_pose = goal_msg.pose\n        nav_goal_msg.original_command = cmd_msg.original_command\n        self.nav_goal_pub.publish(nav_goal_msg)\n\n    def find_object_in_scene(self, object_name):\n        """Find an object in the semantic scene and return its pose"""\n        if self.semantic_scene is None:\n            return None\n\n        for obj in self.semantic_scene.objects:\n            if object_name.lower() in obj.name.lower():\n                # Create a pose for the object\n                pose = Pose()\n                pose.position = obj.position\n                pose.orientation.w = 1.0  # Default orientation\n                return pose\n\n        return None\n\n    def send_navigation_goal(self, goal_msg):\n        """Send navigation goal to Nav2"""\n        self.nav_client.wait_for_server()\n\n        # Send the goal asynchronously\n        send_goal_future = self.nav_client.send_goal_async(goal_msg)\n        send_goal_future.add_done_callback(self.goal_response_callback)\n\n    def goal_response_callback(self, future):\n        """Handle goal response"""\n        goal_handle = future.result()\n        if not goal_handle.accepted:\n            self.get_logger().info(\'Navigation goal rejected by server\')\n            return\n\n        self.get_logger().info(\'Navigation goal accepted by server\')\n        get_result_future = goal_handle.get_result_async()\n        get_result_future.add_done_callback(self.get_result_callback)\n\n    def get_result_callback(self, future):\n        """Handle navigation result"""\n        try:\n            result = future.result().result\n            self.get_logger().info(f\'Navigation completed with result: {result}\')\n        except Exception as e:\n            self.get_logger().error(f\'Navigation failed with error: {e}\')\n\n    def navigate_to_pose_callback(self, request, response):\n        """Service callback for navigation"""\n        try:\n            goal_msg = NavigateToPoseAction.Goal()\n            goal_msg.pose = request.target_pose\n\n            # Check if we can navigate (not too frequent)\n            current_time = self.get_clock().now()\n            time_diff = (current_time - self.last_navigation_time).nanoseconds / 1e9\n            if time_diff < 1.0:  # Minimum 1 second between navigation requests\n                response.success = False\n                response.message = "Navigation request too frequent"\n                return response\n\n            self.nav_client.wait_for_server()\n            send_goal_future = self.nav_client.send_goal_async(goal_msg)\n\n            # For this example, we\'ll return success immediately\n            # In a real system, you\'d wait for completion or implement async handling\n            response.success = True\n            response.message = "Navigation goal sent successfully"\n            self.last_navigation_time = current_time\n\n        except Exception as e:\n            response.success = False\n            response.message = f\'Error during navigation: {str(e)}\'\n\n        return response\n\n    def calculate_distance(self, pose1, pose2):\n        """Calculate Euclidean distance between two poses"""\n        dx = pose1.position.x - pose2.position.x\n        dy = pose1.position.y - pose2.position.y\n        dz = pose1.position.z - pose2.position.z\n        return math.sqrt(dx*dx + dy*dy + dz*dz)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = AdvancedNavigationNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info(\'Shutting down advanced navigation node\')\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(n.h2,{id:"complete-manipulation-system-example",children:"Complete Manipulation System Example"}),"\n",(0,a.jsx)(n.h3,{id:"manipulation-node-with-grasp-planning",children:"Manipulation Node with Grasp Planning"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n# File: humanoid_manipulation/humanoid_manipulation/advanced_manipulation_node.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom humanoid_msgs.msg import ParsedCommand, DetectedObject, SemanticScene\nfrom humanoid_msgs.srv import ManipulateObject\nfrom geometry_msgs.msg import Pose, Point, Quaternion\nfrom trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint\nfrom control_msgs.msg import JointTrajectoryControllerState\nfrom builtin_interfaces.msg import Duration\nimport json\nimport math\n\nclass AdvancedManipulationNode(Node):\n    def __init__(self):\n        super().__init__('advanced_manipulation_node')\n\n        # Publishers and subscribers\n        self.command_sub = self.create_subscription(\n            ParsedCommand, '/parsed_commands', self.command_callback, 10)\n        self.objects_sub = self.create_subscription(\n            SemanticScene, '/semantic_scene', self.semantic_scene_callback, 10)\n        self.joint_state_sub = self.create_subscription(\n            JointTrajectoryControllerState, '/joint_states', self.joint_state_callback, 10)\n\n        # Service for manipulation\n        self.manip_srv = self.create_service(\n            ManipulateObject, '/manipulate_object', self.manipulate_callback)\n\n        # Trajectory publisher for arm control\n        self.arm_traj_pub = self.create_publisher(\n            JointTrajectory, '/arm_controller/joint_trajectory', 10)\n        self.gripper_pub = self.create_publisher(\n            JointTrajectory, '/gripper_controller/joint_trajectory', 10)\n\n        # Detected objects storage\n        self.semantic_scene = None\n        self.joint_states = None\n\n        # Robot arm parameters (simplified)\n        self.arm_joints = ['shoulder_pan_joint', 'shoulder_lift_joint',\n                          'elbow_joint', 'wrist_1_joint', 'wrist_2_joint', 'wrist_3_joint']\n        self.gripper_joints = ['left_gripper_joint', 'right_gripper_joint']\n\n        # Default arm configuration\n        self.default_arm_pose = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n        self.default_gripper_pose = [0.0, 0.0]  # Open\n\n        self.get_logger().info('Advanced Manipulation Node initialized')\n\n    def semantic_scene_callback(self, msg):\n        \"\"\"Callback for semantic scene updates\"\"\"\n        self.semantic_scene = msg\n        self.get_logger().info(f'Updated semantic scene with {len(msg.objects)} objects')\n\n    def joint_state_callback(self, msg):\n        \"\"\"Callback for joint states\"\"\"\n        self.joint_states = msg\n\n    def command_callback(self, msg):\n        \"\"\"Callback for parsed commands\"\"\"\n        if msg.action_type in ['manipulation', 'combined']:\n            self.handle_manipulation_command(msg)\n\n    def handle_manipulation_command(self, cmd_msg):\n        \"\"\"Handle manipulation commands\"\"\"\n        target_object = cmd_msg.target_object.lower()\n\n        if not self.semantic_scene:\n            self.get_logger().warn('No semantic scene available for manipulation')\n            return\n\n        # Find the object in detected objects\n        object_to_manipulate = None\n        for obj in self.semantic_scene.objects:\n            if target_object in obj.name.lower():\n                object_to_manipulate = obj\n                break\n\n        if object_to_manipulate:\n            self.get_logger().info(f'Attempting to manipulate {object_to_manipulate.name}')\n\n            # Plan and execute manipulation\n            success = self.execute_manipulation(object_to_manipulate, cmd_msg.original_command)\n\n            if success:\n                self.get_logger().info(f'Successfully manipulated {object_to_manipulate.name}')\n            else:\n                self.get_logger().warn(f'Failed to manipulate {object_to_manipulate.name}')\n        else:\n            self.get_logger().warn(f'Object {target_object} not found in scene')\n\n    def execute_manipulation(self, obj, command):\n        \"\"\"Execute manipulation based on command and object\"\"\"\n        # Parse the command to determine the action\n        command_lower = command.lower()\n\n        if 'pick' in command_lower or 'grasp' in command_lower or 'grab' in command_lower:\n            return self.execute_pick(obj)\n        elif 'place' in command_lower or 'put' in command_lower:\n            # For place, we need a target location\n            target_location = self.extract_location_from_command(command)\n            if target_location:\n                return self.execute_place(obj, target_location)\n            else:\n                return self.execute_place(obj, 'default')\n        elif 'move' in command_lower:\n            target_location = self.extract_location_from_command(command)\n            if target_location:\n                return self.execute_move(obj, target_location)\n\n        self.get_logger().warn(f'Unknown manipulation action in command: {command}')\n        return False\n\n    def extract_location_from_command(self, command):\n        \"\"\"Extract target location from command\"\"\"\n        # Simple keyword-based extraction\n        command_lower = command.lower()\n\n        location_keywords = {\n            'kitchen': ['kitchen', 'counter', 'table'],\n            'living_room': ['living room', 'couch', 'sofa'],\n            'bedroom': ['bedroom', 'bed'],\n            'office': ['office', 'desk'],\n            'table': ['table', 'surface'],\n            'shelf': ['shelf', 'cabinet']\n        }\n\n        for location, keywords in location_keywords.items():\n            for keyword in keywords:\n                if keyword in command_lower:\n                    return location\n\n        return None\n\n    def execute_pick(self, obj):\n        \"\"\"Execute pick action for an object\"\"\"\n        try:\n            # 1. Plan approach trajectory to the object\n            approach_pose = self.calculate_approach_pose(obj)\n            if not approach_pose:\n                return False\n\n            # 2. Move arm to approach position\n            if not self.move_arm_to_pose(approach_pose):\n                return False\n\n            # 3. Calculate grasp pose (slightly above the object)\n            grasp_pose = self.calculate_grasp_pose(obj)\n            if not grasp_pose:\n                return False\n\n            # 4. Move arm to grasp position\n            if not self.move_arm_to_pose(grasp_pose):\n                return False\n\n            # 5. Close gripper to grasp the object\n            if not self.close_gripper():\n                return False\n\n            # 6. Lift the object slightly\n            lift_pose = self.calculate_lift_pose(grasp_pose)\n            if not self.move_arm_to_pose(lift_pose):\n                return False\n\n            self.get_logger().info(f'Successfully picked up {obj.name}')\n            return True\n\n        except Exception as e:\n            self.get_logger().error(f'Error during pick operation: {e}')\n            return False\n\n    def execute_place(self, obj, location):\n        \"\"\"Execute place action for an object\"\"\"\n        try:\n            # Determine placement location based on the location parameter\n            placement_pose = self.calculate_placement_pose(location)\n            if not placement_pose:\n                return False\n\n            # Move to placement location\n            if not self.move_arm_to_pose(placement_pose):\n                return False\n\n            # Open gripper to release object\n            if not self.open_gripper():\n                return False\n\n            # Move arm up slightly to clear the placed object\n            clear_pose = self.calculate_clear_pose(placement_pose)\n            if not self.move_arm_to_pose(clear_pose):\n                return False\n\n            self.get_logger().info(f'Successfully placed {obj.name} at {location}')\n            return True\n\n        except Exception as e:\n            self.get_logger().error(f'Error during place operation: {e}')\n            return False\n\n    def execute_move(self, obj, target_location):\n        \"\"\"Execute move action for an object\"\"\"\n        try:\n            # First pick up the object\n            if not self.execute_pick(obj):\n                return False\n\n            # Then place it at the target location\n            return self.execute_place(obj, target_location)\n\n        except Exception as e:\n            self.get_logger().error(f'Error during move operation: {e}')\n            return False\n\n    def calculate_approach_pose(self, obj):\n        \"\"\"Calculate approach pose for an object\"\"\"\n        # Approach from a safe distance above and in front of the object\n        approach_pose = Pose()\n        approach_pose.position.x = obj.position.x\n        approach_pose.position.y = obj.position.y\n        approach_pose.position.z = obj.position.z + 0.2  # 20cm above\n        approach_pose.orientation.w = 1.0  # Default orientation\n\n        return approach_pose\n\n    def calculate_grasp_pose(self, obj):\n        \"\"\"Calculate grasp pose for an object\"\"\"\n        # Position gripper at the object's location\n        grasp_pose = Pose()\n        grasp_pose.position.x = obj.position.x\n        grasp_pose.position.y = obj.position.y\n        grasp_pose.position.z = obj.position.z + obj.dimensions.z/2  # Half the height\n        grasp_pose.orientation.w = 1.0  # Default orientation\n\n        return grasp_pose\n\n    def calculate_lift_pose(self, grasp_pose):\n        \"\"\"Calculate lift pose after grasping\"\"\"\n        lift_pose = Pose()\n        lift_pose.position.x = grasp_pose.position.x\n        lift_pose.position.y = grasp_pose.position.y\n        lift_pose.position.z = grasp_pose.position.z + 0.1  # Lift 10cm\n        lift_pose.orientation.w = grasp_pose.orientation.w\n\n        return lift_pose\n\n    def calculate_placement_pose(self, location):\n        \"\"\"Calculate placement pose based on location\"\"\"\n        # Simplified placement positions for different locations\n        placement_positions = {\n            'kitchen': Point(x=1.5, y=0.5, z=0.8),  # Kitchen counter\n            'living_room': Point(x=-0.5, y=-0.5, z=0.6),  # Coffee table\n            'bedroom': Point(x=0.5, y=-1.5, z=0.7),  # Bedside table\n            'office': Point(x=1.0, y=-0.5, z=0.75),  # Desk\n            'table': Point(x=0.0, y=0.0, z=0.75),  # Default table\n            'shelf': Point(x=1.0, y=0.0, z=1.2),  # Shelf\n            'default': Point(x=0.0, y=0.0, z=0.75)  # Default position\n        }\n\n        if location in placement_positions:\n            placement_pose = Pose()\n            placement_pose.position = placement_positions[location]\n            placement_pose.orientation.w = 1.0\n            return placement_pose\n        else:\n            self.get_logger().warn(f'Unknown placement location: {location}, using default')\n            default_pose = Pose()\n            default_pose.position = placement_positions['default']\n            default_pose.orientation.w = 1.0\n            return default_pose\n\n    def calculate_clear_pose(self, placement_pose):\n        \"\"\"Calculate clear pose after placing\"\"\"\n        clear_pose = Pose()\n        clear_pose.position.x = placement_pose.position.x\n        clear_pose.position.y = placement_pose.position.y\n        clear_pose.position.z = placement_pose.position.z + 0.1  # Lift slightly\n        clear_pose.orientation.w = placement_pose.orientation.w\n\n        return clear_pose\n\n    def move_arm_to_pose(self, pose):\n        \"\"\"Move the robot arm to a specific pose\"\"\"\n        try:\n            # This is a simplified trajectory planning\n            # In a real system, you'd use inverse kinematics to calculate joint angles\n            trajectory_msg = JointTrajectory()\n            trajectory_msg.joint_names = self.arm_joints\n\n            # Create a single trajectory point (simplified)\n            point = JointTrajectoryPoint()\n\n            # For this example, we'll use a simple approach\n            # In reality, you'd calculate the joint angles using IK\n            point.positions = self.default_arm_pose.copy()  # Placeholder\n\n            # Set timing\n            point.time_from_start.sec = 2\n            point.time_from_start.nanosec = 0\n\n            trajectory_msg.points.append(point)\n\n            # Publish the trajectory\n            self.arm_traj_pub.publish(trajectory_msg)\n\n            # Wait for completion (simplified)\n            self.get_clock().sleep_for(Duration(nanosec=2000000000))  # 2 seconds\n\n            self.get_logger().info(f'Moved arm to pose: ({pose.position.x}, {pose.position.y}, {pose.position.z})')\n            return True\n\n        except Exception as e:\n            self.get_logger().error(f'Error moving arm to pose: {e}')\n            return False\n\n    def close_gripper(self):\n        \"\"\"Close the robot gripper\"\"\"\n        try:\n            trajectory_msg = JointTrajectory()\n            trajectory_msg.joint_names = self.gripper_joints\n\n            point = JointTrajectoryPoint()\n            # Close gripper (simplified - actual values depend on gripper design)\n            point.positions = [0.02, 0.02]  # Closed position\n            point.time_from_start.sec = 1\n            point.time_from_start.nanosec = 0\n\n            trajectory_msg.points.append(point)\n\n            self.gripper_pub.publish(trajectory_msg)\n\n            # Wait for completion\n            self.get_clock().sleep_for(Duration(nanosec=1000000000))  # 1 second\n\n            self.get_logger().info('Closed gripper')\n            return True\n\n        except Exception as e:\n            self.get_logger().error(f'Error closing gripper: {e}')\n            return False\n\n    def open_gripper(self):\n        \"\"\"Open the robot gripper\"\"\"\n        try:\n            trajectory_msg = JointTrajectory()\n            trajectory_msg.joint_names = self.gripper_joints\n\n            point = JointTrajectoryPoint()\n            # Open gripper\n            point.positions = [0.08, 0.08]  # Open position\n            point.time_from_start.sec = 1\n            point.time_from_start.nanosec = 0\n\n            trajectory_msg.points.append(point)\n\n            self.gripper_pub.publish(trajectory_msg)\n\n            # Wait for completion\n            self.get_clock().sleep_for(Duration(nanosec=1000000000))  # 1 second\n\n            self.get_logger().info('Opened gripper')\n            return True\n\n        except Exception as e:\n            self.get_logger().error(f'Error opening gripper: {e}')\n            return False\n\n    def manipulate_callback(self, request, response):\n        \"\"\"Service callback for manipulation\"\"\"\n        try:\n            # In a real system, you would implement the full manipulation pipeline\n            if request.action == 'pick':\n                # Find the object in the semantic scene\n                if self.semantic_scene:\n                    for obj in self.semantic_scene.objects:\n                        if request.object_name.lower() in obj.name.lower():\n                            response.success = self.execute_pick(obj)\n                            break\n                    else:\n                        response.success = False\n                        response.message = f'Object {request.object_name} not found'\n                else:\n                    response.success = False\n                    response.message = 'No semantic scene available'\n\n            elif request.action == 'place':\n                # For place, we'd need to find a carried object or implement differently\n                response.success = True  # Simplified for example\n                response.message = f'Successfully placed object at {request.location}'\n\n            elif request.action == 'move':\n                response.success = True  # Simplified for example\n                response.message = f'Successfully moved {request.object_name} to {request.location}'\n\n            else:\n                response.success = False\n                response.message = f'Unknown action: {request.action}'\n\n        except Exception as e:\n            response.success = False\n            response.message = f'Error during manipulation: {str(e)}'\n\n        if response.success:\n            response.message = f'Successfully executed {request.action} action'\n        else:\n            response.message = f'Failed to execute {request.action} action: {response.message}'\n\n        self.get_logger().info(response.message)\n        return response\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = AdvancedManipulationNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info('Shutting down advanced manipulation node')\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"complete-system-integration-example",children:"Complete System Integration Example"}),"\n",(0,a.jsx)(n.h3,{id:"main-integration-node",children:"Main Integration Node"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n# File: humanoid_system_integration/humanoid_system_integration/integration_node.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom humanoid_msgs.msg import ParsedCommand, TaskPlan, TaskStep\nfrom humanoid_msgs.srv import CommandValidation\nfrom std_msgs.msg import String, Bool\nfrom builtin_interfaces.msg import Time\nimport json\nimport time\n\nclass SystemIntegrationNode(Node):\n    def __init__(self):\n        super().__init__('system_integration_node')\n\n        # Publishers and subscribers\n        self.command_sub = self.create_subscription(\n            ParsedCommand, '/parsed_commands', self.command_callback, 10)\n        self.plan_sub = self.create_subscription(\n            TaskPlan, '/task_plans', self.plan_callback, 10)\n\n        self.system_status_pub = self.create_publisher(\n            String, '/system_status', 10)\n        self.task_status_pub = self.create_publisher(\n            String, '/task_status', 10)\n\n        # Service clients for various subsystems\n        self.nav_client = self.create_client(\n            CommandValidation, '/validate_command')  # Placeholder for navigation service\n        self.manip_client = self.create_client(\n            CommandValidation, '/validate_command')  # Placeholder for manipulation service\n\n        # System state\n        self.system_ready = False\n        self.current_task = None\n        self.task_queue = []\n        self.subsystem_status = {\n            'voice': False,\n            'perception': False,\n            'navigation': False,\n            'manipulation': False,\n            'planning': False\n        }\n\n        # Timer for system monitoring\n        self.monitor_timer = self.create_timer(1.0, self.monitor_system)\n\n        self.get_logger().info('System Integration Node initialized')\n\n    def command_callback(self, msg):\n        \"\"\"Callback for parsed commands\"\"\"\n        self.get_logger().info(f'Received command: {msg.original_command} ({msg.action_type})')\n\n        # Validate command\n        if self.validate_command(msg):\n            # Create and execute task plan\n            task_plan = self.create_task_plan(msg)\n            if task_plan:\n                self.execute_task_plan(task_plan)\n\n    def plan_callback(self, msg):\n        \"\"\"Callback for task plans\"\"\"\n        self.get_logger().info(f'Received task plan with {len(msg.steps)} steps')\n\n    def validate_command(self, parsed_cmd):\n        \"\"\"Validate command using subsystem validation\"\"\"\n        # For this example, we'll do basic validation\n        if not parsed_cmd.original_command.strip():\n            self.get_logger().warn('Empty command received')\n            return False\n\n        # Check if system is ready\n        if not self.system_ready:\n            self.get_logger().warn('System not ready to process commands')\n            return False\n\n        # Check if all required subsystems are available\n        required_subsystems = []\n        if parsed_cmd.action_type in ['navigation', 'combined']:\n            required_subsystems.append('navigation')\n        if parsed_cmd.action_type in ['manipulation', 'combined']:\n            required_subsystems.append('manipulation')\n\n        for subsystem in required_subsystems:\n            if not self.subsystem_status.get(subsystem, False):\n                self.get_logger().warn(f'Required subsystem {subsystem} not available')\n                return False\n\n        self.get_logger().info(f'Command validated: {parsed_cmd.original_command}')\n        return True\n\n    def create_task_plan(self, parsed_cmd):\n        \"\"\"Create a task plan from parsed command\"\"\"\n        task_plan = TaskPlan()\n        task_plan.header.stamp = self.get_clock().now().to_msg()\n        task_plan.header.frame_id = 'map'\n        task_plan.original_command = parsed_cmd.original_command\n        task_plan.command_type = parsed_cmd.action_type\n\n        # Based on action type, create appropriate task steps\n        if parsed_cmd.action_type == 'navigation':\n            # Navigation task\n            nav_step = TaskStep()\n            nav_step.action_type = 'navigation'\n            nav_step.target_location = parsed_cmd.target_location\n            nav_step.parameters = parsed_cmd.parameters\n            nav_step.description = f'Navigate to {parsed_cmd.target_location}'\n            task_plan.steps.append(nav_step)\n\n        elif parsed_cmd.action_type == 'manipulation':\n            # Manipulation task - might need navigation first\n            if parsed_cmd.target_location:\n                # Navigate to location first\n                nav_step = TaskStep()\n                nav_step.action_type = 'navigation'\n                nav_step.target_location = parsed_cmd.target_location\n                nav_step.parameters = parsed_cmd.parameters\n                nav_step.description = f'Navigate to {parsed_cmd.target_location}'\n                task_plan.steps.append(nav_step)\n\n            # Then manipulate object\n            manip_step = TaskStep()\n            manip_step.action_type = 'manipulation'\n            manip_step.target_object = parsed_cmd.target_object\n            manip_step.parameters = parsed_cmd.parameters\n            manip_step.description = f'Manipulate {parsed_cmd.target_object}'\n            task_plan.steps.append(manip_step)\n\n        elif parsed_cmd.action_type == 'combined':\n            # Combined task - navigate and manipulate\n            nav_step = TaskStep()\n            nav_step.action_type = 'navigation'\n            nav_step.target_location = parsed_cmd.target_location\n            nav_step.description = f'Navigate to {parsed_cmd.target_location}'\n            task_plan.steps.append(nav_step)\n\n            manip_step = TaskStep()\n            manip_step.action_type = 'manipulation'\n            manip_step.target_object = parsed_cmd.target_object\n            manip_step.description = f'Manipulate {parsed_cmd.target_object}'\n            task_plan.steps.append(manip_step)\n\n        elif parsed_cmd.action_type == 'query':\n            # Query task - perception only\n            query_step = TaskStep()\n            query_step.action_type = 'perception'\n            query_step.target_object = parsed_cmd.target_object\n            query_step.description = f'Query for {parsed_cmd.target_object}'\n            task_plan.steps.append(query_step)\n\n        else:\n            self.get_logger().warn(f'Unknown action type: {parsed_cmd.action_type}')\n            return None\n\n        # Publish the task plan\n        plan_pub = self.create_publisher(TaskPlan, '/task_plans', 10)\n        plan_pub.publish(task_plan)\n\n        self.get_logger().info(f'Created task plan with {len(task_plan.steps)} steps')\n        return task_plan\n\n    def execute_task_plan(self, task_plan):\n        \"\"\"Execute the task plan step by step\"\"\"\n        self.get_logger().info(f'Executing task plan: {task_plan.original_command}')\n        self.current_task = task_plan\n\n        # Update task status\n        status_msg = String()\n        status_msg.data = f'Executing task: {task_plan.original_command}'\n        self.task_status_pub.publish(status_msg)\n\n        # Execute each step in the plan\n        for i, step in enumerate(task_plan.steps):\n            self.get_logger().info(f'Executing step {i+1}/{len(task_plan.steps)}: {step.description}')\n\n            success = self.execute_task_step(step)\n            if not success:\n                self.get_logger().error(f'Task step {i+1} failed: {step.description}')\n                # For this example, we'll continue with other steps, but in a real system\n                # you might want to implement different failure handling strategies\n                continue\n\n        # Task completed\n        self.get_logger().info('Task plan execution completed')\n        self.current_task = None\n\n        # Update status\n        status_msg = String()\n        status_msg.data = 'Task completed'\n        self.task_status_pub.publish(status_msg)\n\n    def execute_task_step(self, step):\n        \"\"\"Execute a single task step\"\"\"\n        try:\n            if step.action_type == 'navigation':\n                return self.execute_navigation_step(step)\n            elif step.action_type == 'manipulation':\n                return self.execute_manipulation_step(step)\n            elif step.action_type == 'perception':\n                return self.execute_perception_step(step)\n            else:\n                self.get_logger().warn(f'Unknown step type: {step.action_type}')\n                return False\n\n        except Exception as e:\n            self.get_logger().error(f'Error executing task step: {e}')\n            return False\n\n    def execute_navigation_step(self, step):\n        \"\"\"Execute navigation step\"\"\"\n        self.get_logger().info(f'Navigating to {step.target_location}')\n\n        # In a real system, you would call the navigation service\n        # For this example, we'll simulate the navigation\n        time.sleep(2)  # Simulate navigation time\n\n        self.get_logger().info(f'Reached {step.target_location}')\n        return True\n\n    def execute_manipulation_step(self, step):\n        \"\"\"Execute manipulation step\"\"\"\n        self.get_logger().info(f'Manipulating {step.target_object}')\n\n        # In a real system, you would call the manipulation service\n        # For this example, we'll simulate the manipulation\n        time.sleep(3)  # Simulate manipulation time\n\n        self.get_logger().info(f'Finished manipulating {step.target_object}')\n        return True\n\n    def execute_perception_step(self, step):\n        \"\"\"Execute perception step\"\"\"\n        self.get_logger().info(f'Perceiving {step.target_object}')\n\n        # In a real system, you would call the perception service\n        # For this example, we'll simulate the perception\n        time.sleep(1)  # Simulate perception time\n\n        self.get_logger().info(f'Finished perceiving {step.target_object}')\n        return True\n\n    def monitor_system(self):\n        \"\"\"Monitor system status and subsystem availability\"\"\"\n        # In a real system, you would check the actual status of each subsystem\n        # For this example, we'll simulate status checking\n\n        # Simulate checking if subsystems are responding\n        self.subsystem_status['voice'] = True  # Assume voice system is always available\n        self.subsystem_status['perception'] = True  # Assume perception is available\n        self.subsystem_status['navigation'] = True  # Assume navigation is available\n        self.subsystem_status['manipulation'] = True  # Assume manipulation is available\n        self.subsystem_status['planning'] = True  # Assume planning is available\n\n        # Update system readiness based on subsystem status\n        self.system_ready = all(self.subsystem_status.values())\n\n        # Publish system status\n        status_msg = String()\n        status_msg.data = f\"System Ready: {self.system_ready}, Subsystems: {self.subsystem_status}\"\n        self.system_status_pub.publish(status_msg)\n\n        # Log system status periodically\n        if self.system_ready:\n            self.get_logger().info('System is ready to process commands')\n        else:\n            self.get_logger().info(f'System not ready. Status: {self.subsystem_status}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = SystemIntegrationNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info('Shutting down system integration node')\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"launch-files-and-configuration",children:"Launch Files and Configuration"}),"\n",(0,a.jsx)(n.h3,{id:"main-launch-file-for-complete-system",children:"Main Launch File for Complete System"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:"\x3c!-- File: humanoid_bringup/launch/complete_autonomous_humanoid.launch.py --\x3e\nfrom launch import LaunchDescription\nfrom launch.actions import IncludeLaunchDescription, DeclareLaunchArgument\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch_ros.actions import Node\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    # Declare launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time', default='true')\n    robot_namespace = LaunchConfiguration('robot_namespace', default='humanoid_robot')\n\n    # Get package directories\n    gazebo_ros_package_dir = get_package_share_directory('gazebo_ros')\n    humanoid_description_dir = get_package_share_directory('humanoid_description')\n    humanoid_voice_dir = get_package_share_directory('humanoid_voice_control')\n    humanoid_simulation_dir = get_package_share_directory('humanoid_simulation')\n\n    return LaunchDescription([\n        # Declare launch arguments\n        DeclareLaunchArgument(\n            'use_sim_time',\n            default_value='true',\n            description='Use simulation (Gazebo) clock if true'\n        ),\n        DeclareLaunchArgument(\n            'robot_namespace',\n            default_value='humanoid_robot',\n            description='Robot namespace'\n        ),\n\n        # Start Gazebo server with a world file\n        IncludeLaunchDescription(\n            PythonLaunchDescriptionSource(\n                os.path.join(gazebo_ros_package_dir, 'launch', 'gzserver.launch.py')\n            ),\n            launch_arguments={\n                'world': PathJoinSubstitution([\n                    humanoid_simulation_dir,\n                    'worlds',\n                    'humanoid_world.sdf'\n                ])\n            }.items()\n        ),\n\n        # Start Gazebo client\n        IncludeLaunchDescription(\n            PythonLaunchDescriptionSource(\n                os.path.join(gazebo_ros_package_dir, 'launch', 'gzclient.launch.py')\n            )\n        ),\n\n        # Spawn the humanoid robot in Gazebo\n        Node(\n            package='gazebo_ros',\n            executable='spawn_entity.py',\n            arguments=[\n                '-entity', [robot_namespace],\n                '-topic', 'robot_description',\n                '-x', '0.0',\n                '-y', '0.0',\n                '-z', '0.0'\n            ],\n            output='screen'\n        ),\n\n        # Robot state publisher\n        Node(\n            package='robot_state_publisher',\n            executable='robot_state_publisher',\n            output='screen',\n            parameters=[\n                {'use_sim_time': use_sim_time},\n                {'robot_description':\n                    open(os.path.join(humanoid_description_dir, 'urdf', 'humanoid.urdf')).read()}\n            ]\n        ),\n\n        # Joint state publisher\n        Node(\n            package='joint_state_publisher',\n            executable='joint_state_publisher',\n            output='screen',\n            parameters=[\n                {'use_sim_time': use_sim_time}\n            ]\n        ),\n\n        # Start voice control system\n        IncludeLaunchDescription(\n            PythonLaunchDescriptionSource(\n                os.path.join(humanoid_voice_dir, 'launch', 'voice_system.launch.py')\n            )\n        ),\n\n        # Start perception system\n        Node(\n            package='humanoid_perception',\n            executable='advanced_perception_node',\n            name='advanced_perception_node',\n            output='screen',\n            parameters=[\n                {'use_sim_time': use_sim_time}\n            ]\n        ),\n\n        # Start navigation system\n        Node(\n            package='humanoid_navigation',\n            executable='advanced_navigation_node',\n            name='advanced_navigation_node',\n            output='screen',\n            parameters=[\n                {'use_sim_time': use_sim_time}\n            ]\n        ),\n\n        # Start manipulation system\n        Node(\n            package='humanoid_manipulation',\n            executable='advanced_manipulation_node',\n            name='advanced_manipulation_node',\n            output='screen',\n            parameters=[\n                {'use_sim_time': use_sim_time}\n            ]\n        ),\n\n        # Start task planning system\n        Node(\n            package='humanoid_planning',\n            executable='task_planning_node',\n            name='task_planning_node',\n            output='screen',\n            parameters=[\n                {'use_sim_time': use_sim_time}\n            ]\n        ),\n\n        # Start system integration node\n        Node(\n            package='humanoid_system_integration',\n            executable='integration_node',\n            name='integration_node',\n            output='screen',\n            parameters=[\n                {'use_sim_time': use_sim_time}\n            ]\n        )\n    ])\n"})}),"\n",(0,a.jsx)(n.h2,{id:"testing-and-validation-scripts",children:"Testing and Validation Scripts"}),"\n",(0,a.jsx)(n.h3,{id:"system-test-script",children:"System Test Script"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n# File: test_system_integration.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom humanoid_msgs.msg import ParsedCommand\nimport time\n\nclass SystemTestNode(Node):\n    def __init__(self):\n        super().__init__('system_test_node')\n\n        # Publishers for testing\n        self.voice_cmd_pub = self.create_publisher(String, '/voice_commands', 10)\n        self.parsed_cmd_pub = self.create_publisher(ParsedCommand, '/parsed_commands', 10)\n\n        self.get_logger().info('System Test Node initialized')\n\n        # Schedule test commands\n        self.timer = self.create_timer(5.0, self.run_tests)\n        self.test_counter = 0\n        self.test_commands = [\n            'Go to the kitchen',\n            'Find the red cup',\n            'Pick up the blue ball',\n            'Go to the living room',\n            'Put the ball on the table'\n        ]\n\n    def run_tests(self):\n        \"\"\"Run system integration tests\"\"\"\n        if self.test_counter < len(self.test_commands):\n            command = self.test_commands[self.test_counter]\n\n            # Publish test command\n            cmd_msg = String()\n            cmd_msg.data = command\n            self.voice_cmd_pub.publish(cmd_msg)\n\n            self.get_logger().info(f'Sent test command: {command}')\n\n            self.test_counter += 1\n        else:\n            self.get_logger().info('All tests completed')\n            self.timer.cancel()\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = SystemTestNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info('Shutting down system test node')\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"System Integration"}),": All components must work together seamlessly"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Real-time Processing"}),": System must respond to commands within acceptable timeframes"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Component Coordination"}),": Different subsystems need to communicate effectively"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Error Handling"}),": Robust error handling across all components"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Safety Considerations"}),": Validation of commands before execution"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Modular Design"}),": Each component should be independently testable"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"ROS 2 Communication"}),": Proper use of topics, services, and actions"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"State Management"}),": Tracking system state and task progress"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"practical-exercises",children:"Practical Exercises"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Extend the voice command vocabulary"}),': Add support for new commands like "turn left", "turn right", "stop", etc.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Improve object detection"}),": Integrate a real object detection model (YOLO, SSD, etc.) instead of the simple color-based detection."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Add more locations"}),": Expand the location map with more destinations in the simulated environment."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Implement error recovery"}),": Add mechanisms to handle and recover from failed navigation or manipulation attempts."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Add safety checks"}),": Implement more sophisticated safety validation before executing commands."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Create a GUI interface"}),": Build a simple GUI that displays system status and allows manual command input."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"common-failure-modes",children:"Common Failure Modes"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Integration Failures"}),": Components not properly communicating with each other"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Timing Issues"}),": Delays causing poor user experience or system instability"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Resource Exhaustion"}),": High computational requirements affecting performance"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Safety Violations"}),": Actions executed without proper safety checks"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Error Propagation"}),": Failures in one component affecting others"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Communication Failures"}),": ROS topics/services not connecting properly"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Perception Failures"}),": Object detection or scene understanding errors"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Action Execution Failures"}),": Navigation or manipulation errors"]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(m,{...e})}):m(e)}},8453(e,n,t){t.d(n,{R:()=>i,x:()=>r});var o=t(6540);const a={},s=o.createContext(a);function i(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);