"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[38],{800(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>m,frontMatter:()=>r,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"module-4-vla/code-examples","title":"Code Examples: LLM Integration and ROS 2 Action Mapping","description":"Large Language Model Integration","source":"@site/docs/module-4-vla/code-examples.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/code-examples","permalink":"/docs/module-4-vla/code-examples","draft":false,"unlisted":false,"editUrl":"https://github.com/Farhan899/Ai-Robotics-Book/edit/main/docs/module-4-vla/code-examples.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Hands-On: Vision-Language-Action Implementation","permalink":"/docs/module-4-vla/hands-on"},"next":{"title":"Troubleshooting: Vision-Language-Action Integration Issues","permalink":"/docs/module-4-vla/troubleshooting"}}');var s=t(4848),o=t(8453);const r={},i="Code Examples: LLM Integration and ROS 2 Action Mapping",l={},c=[{value:"Large Language Model Integration",id:"large-language-model-integration",level:2},{value:"OpenAI GPT Integration for Command Understanding",id:"openai-gpt-integration-for-command-understanding",level:3},{value:"Hugging Face Transformers Integration",id:"hugging-face-transformers-integration",level:2},{value:"Local LLM for Command Understanding",id:"local-llm-for-command-understanding",level:3},{value:"Vision-Language Integration",id:"vision-language-integration",level:2},{value:"Vision-Grounded Language Understanding",id:"vision-grounded-language-understanding",level:3},{value:"ROS 2 Action Mapping and Execution",id:"ros-2-action-mapping-and-execution",level:2},{value:"Action Planner and Executor",id:"action-planner-and-executor",level:3},{value:"Whisper Integration for Speech-to-Text",id:"whisper-integration-for-speech-to-text",level:2},{value:"OpenAI Whisper Node",id:"openai-whisper-node",level:3},{value:"Complete VLA System Integration",id:"complete-vla-system-integration",level:2},{value:"Main VLA System Node",id:"main-vla-system-node",level:3},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Practical Exercises",id:"practical-exercises",level:2},{value:"Common Failure Modes",id:"common-failure-modes",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"code-examples-llm-integration-and-ros-2-action-mapping",children:"Code Examples: LLM Integration and ROS 2 Action Mapping"})}),"\n",(0,s.jsx)(n.h2,{id:"large-language-model-integration",children:"Large Language Model Integration"}),"\n",(0,s.jsx)(n.h3,{id:"openai-gpt-integration-for-command-understanding",children:"OpenAI GPT Integration for Command Understanding"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import openai\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom vla_interfaces.msg import ParsedCommand\nimport json\nimport os\n\n\nclass LLMCommandParserNode(Node):\n\n    def __init__(self):\n        super().__init__(\'llm_command_parser_node\')\n\n        # Publishers and Subscribers\n        self.command_sub = self.create_subscription(\n            String, \'voice_commands\', self.command_callback, 10)\n        self.parsed_pub = self.create_publisher(\n            ParsedCommand, \'parsed_commands\', 10)\n\n        # Initialize OpenAI client\n        openai.api_key = os.getenv(\'OPENAI_API_KEY\')\n        if not openai.api_key:\n            self.get_logger().error(\'OPENAI_API_KEY environment variable not set\')\n            return\n\n        # Define system prompt for command parsing\n        self.system_prompt = """\n        You are a command parser for a robotic system. Your job is to interpret natural language commands\n        and convert them into structured robot commands.\n\n        Available actions:\n        - navigate_to: Move the robot to a location\n        - grasp_object: Pick up an object\n        - place_object: Place an object at a location\n        - detect_object: Find an object in the environment\n        - follow_person: Follow a person\n        - report_status: Report robot status\n\n        Response format:\n        {\n            "action": "action_name",\n            "target_object": "object_name_or_null",\n            "target_location": "location_name_or_null",\n            "parameters": {"key": "value"}\n        }\n\n        Examples:\n        Input: "Please go to the kitchen and get me a cup"\n        Output: {"action": "navigate_to", "target_location": "kitchen", "target_object": null, "parameters": {}}\n\n        Input: "Pick up the red cup"\n        Output: {"action": "grasp_object", "target_object": "red cup", "target_location": null, "parameters": {}}\n        """\n\n        self.get_logger().info(\'LLM Command Parser Node initialized\')\n\n    def command_callback(self, msg):\n        """Process incoming voice commands using LLM"""\n        command_text = msg.data\n        self.get_logger().info(f\'Processing command with LLM: {command_text}\')\n\n        try:\n            response = openai.ChatCompletion.create(\n                model="gpt-3.5-turbo",\n                messages=[\n                    {"role": "system", "content": self.system_prompt},\n                    {"role": "user", "content": command_text}\n                ],\n                temperature=0.1,  # Low temperature for more consistent parsing\n                max_tokens=150\n            )\n\n            # Extract and parse the response\n            response_text = response.choices[0].message.content.strip()\n\n            # Clean up the response (remove any markdown formatting)\n            if response_text.startswith(\'```json\'):\n                response_text = response_text[7:]  # Remove ```json\n            if response_text.endswith(\'```\'):\n                response_text = response_text[:-3]  # Remove ```\n\n            # Parse the JSON response\n            parsed_data = json.loads(response_text)\n\n            # Create and publish parsed command\n            parsed_cmd = ParsedCommand()\n            parsed_cmd.action = parsed_data.get(\'action\', \'unknown\')\n            parsed_cmd.target_object = parsed_data.get(\'target_object\', \'\')\n            parsed_cmd.target_location = parsed_data.get(\'target_location\', \'\')\n            parsed_cmd.raw_command = command_text\n            parsed_cmd.parameters = json.dumps(parsed_data.get(\'parameters\', {}))\n\n            self.parsed_pub.publish(parsed_cmd)\n            self.get_logger().info(f\'LLM parsed command: {parsed_cmd.action}\')\n\n        except json.JSONDecodeError:\n            self.get_logger().error(f\'LLM returned invalid JSON: {response_text}\')\n        except Exception as e:\n            self.get_logger().error(f\'Error processing command with LLM: {e}\')\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = LLMCommandParserNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"hugging-face-transformers-integration",children:"Hugging Face Transformers Integration"}),"\n",(0,s.jsx)(n.h3,{id:"local-llm-for-command-understanding",children:"Local LLM for Command Understanding"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom vla_interfaces.msg import ParsedCommand\nfrom transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport json\n\n\nclass LocalLLMCommandParserNode(Node):\n\n    def __init__(self):\n        super().__init__('local_llm_command_parser_node')\n\n        # Publishers and Subscribers\n        self.command_sub = self.create_subscription(\n            String, 'voice_commands', self.command_callback, 10)\n        self.parsed_pub = self.create_publisher(\n            ParsedCommand, 'parsed_commands', 10)\n\n        # Load a pre-trained model (using a smaller model for efficiency)\n        model_name = \"microsoft/DialoGPT-medium\"  # Example model\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n\n        # Add padding token if it doesn't exist\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n\n        # Create text generation pipeline\n        self.generator = pipeline(\n            'text-generation',\n            model=self.model,\n            tokenizer=self.tokenizer,\n            device=0 if torch.cuda.is_available() else -1\n        )\n\n        # Define command templates\n        self.command_template = \"\"\"\n        You are a robot command parser. Convert the following command into structured format:\n        Command: {command}\n\n        Available actions: navigate_to, grasp_object, place_object, detect_object, follow_person, report_status\n\n        Format your response as JSON:\n        {{\n            \"action\": \"action_name\",\n            \"target_object\": \"object_name_or_null\",\n            \"target_location\": \"location_name_or_null\",\n            \"parameters\": {{\"key\": \"value\"}}\n        }}\n        \"\"\"\n\n        self.get_logger().info('Local LLM Command Parser Node initialized')\n\n    def command_callback(self, msg):\n        \"\"\"Process incoming voice commands using local LLM\"\"\"\n        command_text = msg.data\n        self.get_logger().info(f'Processing command with local LLM: {command_text}')\n\n        try:\n            # Create prompt\n            prompt = self.command_template.format(command=command_text)\n\n            # Generate response\n            response = self.generator(\n                prompt,\n                max_length=len(self.tokenizer.encode(prompt)) + 100,\n                num_return_sequences=1,\n                temperature=0.1,\n                pad_token_id=self.tokenizer.eos_token_id\n            )\n\n            # Extract the generated text\n            generated_text = response[0]['generated_text'][len(prompt):]\n\n            # Try to find JSON in the response\n            json_start = generated_text.find('{')\n            json_end = generated_text.rfind('}') + 1\n\n            if json_start != -1 and json_end != 0:\n                json_str = generated_text[json_start:json_end]\n                parsed_data = json.loads(json_str)\n\n                # Create and publish parsed command\n                parsed_cmd = ParsedCommand()\n                parsed_cmd.action = parsed_data.get('action', 'unknown')\n                parsed_cmd.target_object = parsed_data.get('target_object', '')\n                parsed_cmd.target_location = parsed_data.get('target_location', '')\n                parsed_cmd.raw_command = command_text\n                parsed_cmd.parameters = json.dumps(parsed_data.get('parameters', {}))\n\n                self.parsed_pub.publish(parsed_cmd)\n                self.get_logger().info(f'Local LLM parsed command: {parsed_cmd.action}')\n            else:\n                self.get_logger().warn(f'Could not extract JSON from LLM response: {generated_text}')\n\n        except json.JSONDecodeError:\n            self.get_logger().error(f'LLM returned invalid JSON: {generated_text}')\n        except Exception as e:\n            self.get_logger().error(f'Error processing command with local LLM: {e}')\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = LocalLLMCommandParserNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"vision-language-integration",children:"Vision-Language Integration"}),"\n",(0,s.jsx)(n.h3,{id:"vision-grounded-language-understanding",children:"Vision-Grounded Language Understanding"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import String\nfrom vla_interfaces.msg import ParsedCommand\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nimport openai\nimport os\nimport json\n\n\nclass VisionGroundedLLMNode(Node):\n\n    def __init__(self):\n        super().__init__('vision_grounded_llm_node')\n\n        # Publishers and Subscribers\n        self.image_sub = self.create_subscription(\n            Image, '/camera/image_raw', self.image_callback, 10)\n        self.command_sub = self.create_subscription(\n            String, 'voice_commands', self.command_callback, 10)\n        self.parsed_pub = self.create_publisher(\n            ParsedCommand, 'parsed_commands', 10)\n\n        # Initialize components\n        self.bridge = CvBridge()\n        self.current_image = None\n        self.image_timestamp = None\n\n        # Initialize OpenAI client\n        openai.api_key = os.getenv('OPENAI_API_KEY')\n\n        self.get_logger().info('Vision-Grounded LLM Node initialized')\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming camera images\"\"\"\n        try:\n            self.current_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n            self.image_timestamp = msg.header.stamp\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n\n    def command_callback(self, msg):\n        \"\"\"Process voice command with vision grounding\"\"\"\n        command_text = msg.data\n        self.get_logger().info(f'Processing vision-grounded command: {command_text}')\n\n        if self.current_image is None:\n            self.get_logger().warn('No current image for vision grounding')\n            # Fall back to text-only processing\n            self.process_text_only_command(command_text)\n            return\n\n        try:\n            # Convert image to base64 for API\n            _, buffer = cv2.imencode('.jpg', self.current_image)\n            image_base64 = buffer.tobytes().hex()\n\n            # Use OpenAI's vision-capable model (GPT-4 Vision)\n            response = openai.ChatCompletion.create(\n                model=\"gpt-4-vision-preview\",\n                messages=[\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\"type\": \"text\", \"text\": f\"Based on this image, interpret the command: '{command_text}'. What should the robot do? If the command refers to specific objects, identify their locations and characteristics. Respond in JSON format with action, target_object, and target_location.\"},\n                            {\n                                \"type\": \"image_url\",\n                                \"image_url\": {\n                                    \"url\": f\"data:image/jpeg;base64,{image_base64}\"\n                                }\n                            }\n                        ]\n                    }\n                ],\n                max_tokens=300\n            )\n\n            response_text = response.choices[0].message.content.strip()\n\n            # Extract JSON from response\n            json_start = response_text.find('{')\n            json_end = response_text.rfind('}') + 1\n\n            if json_start != -1 and json_end != 0:\n                json_str = response_text[json_start:json_end]\n                parsed_data = json.loads(json_str)\n\n                # Create and publish parsed command\n                parsed_cmd = ParsedCommand()\n                parsed_cmd.action = parsed_data.get('action', 'unknown')\n                parsed_cmd.target_object = parsed_data.get('target_object', '')\n                parsed_cmd.target_location = parsed_data.get('target_location', '')\n                parsed_cmd.raw_command = command_text\n                parsed_cmd.parameters = json.dumps(parsed_data.get('parameters', {}))\n\n                self.parsed_pub.publish(parsed_cmd)\n                self.get_logger().info(f'Vision-grounded command: {parsed_cmd.action}')\n\n        except Exception as e:\n            self.get_logger().error(f'Error in vision-grounded processing: {e}')\n            # Fall back to text-only processing\n            self.process_text_only_command(command_text)\n\n    def process_text_only_command(self, command_text):\n        \"\"\"Process command without vision (fallback)\"\"\"\n        # Simple fallback processing\n        parsed_cmd = ParsedCommand()\n        parsed_cmd.action = 'unknown'\n        parsed_cmd.target_object = ''\n        parsed_cmd.target_location = ''\n        parsed_cmd.raw_command = command_text\n        parsed_cmd.parameters = '{}'\n\n        # Simple keyword-based parsing as fallback\n        command_lower = command_text.lower()\n\n        if any(word in command_lower for word in ['go to', 'navigate to', 'move to']):\n            parsed_cmd.action = 'navigate_to'\n        elif any(word in command_lower for word in ['pick up', 'grasp', 'get', 'take']):\n            parsed_cmd.action = 'grasp_object'\n        elif any(word in command_lower for word in ['place', 'put', 'set down']):\n            parsed_cmd.action = 'place_object'\n        elif any(word in command_lower for word in ['find', 'look for', 'locate']):\n            parsed_cmd.action = 'detect_object'\n\n        self.parsed_pub.publish(parsed_cmd)\n        self.get_logger().info(f'Text-only fallback: {parsed_cmd.action}')\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VisionGroundedLLMNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"ros-2-action-mapping-and-execution",children:"ROS 2 Action Mapping and Execution"}),"\n",(0,s.jsx)(n.h3,{id:"action-planner-and-executor",children:"Action Planner and Executor"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom vla_interfaces.msg import ParsedCommand, TaskPlan, ExecutableAction\nfrom geometry_msgs.msg import Pose, Point\nfrom nav2_msgs.action import NavigateToPose\nfrom rclpy.action import ActionClient\nfrom rclpy.callback_groups import ReentrantCallbackGroup\nfrom rclpy.executors import MultiThreadedExecutor\nimport json\nimport threading\n\n\nclass VLAActionExecutorNode(Node):\n\n    def __init__(self):\n        super().__init__('vla_action_executor_node')\n\n        # Publishers and Subscribers\n        self.plan_sub = self.create_subscription(\n            TaskPlan, 'task_plans', self.plan_callback, 10)\n        self.status_pub = self.create_publisher(\n            String, 'action_status', 10)\n\n        # Action clients\n        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')\n\n        # Task execution state\n        self.current_plan = None\n        self.current_task_index = 0\n        self.is_executing = False\n        self.execution_lock = threading.Lock()\n\n        # Timer for executing tasks\n        self.timer = self.create_timer(0.1, self.execute_next_task)\n\n        self.get_logger().info('VLA Action Executor Node initialized')\n\n    def plan_callback(self, msg):\n        \"\"\"Receive and queue task plans for execution\"\"\"\n        with self.execution_lock:\n            if not self.is_executing:\n                self.current_plan = msg\n                self.current_task_index = 0\n                self.is_executing = True\n                self.get_logger().info(f'New plan received with {len(msg.tasks)} tasks')\n            else:\n                self.get_logger().info('Plan queued, current plan still executing')\n\n    def execute_next_task(self):\n        \"\"\"Execute the next task in the current plan\"\"\"\n        if not self.is_executing or self.current_plan is None:\n            return\n\n        if self.current_task_index >= len(self.current_plan.tasks):\n            # Plan completed\n            self.plan_completed()\n            return\n\n        current_task = self.current_plan.tasks[self.current_task_index]\n        self.get_logger().info(f'Executing task {self.current_task_index + 1}: {current_task.action}')\n\n        # Execute based on task type\n        if current_task.action == 'navigate_to':\n            self.execute_navigation_task(current_task)\n        elif current_task.action == 'grasp_object':\n            self.execute_grasp_task(current_task)\n        elif current_task.action == 'place_object':\n            self.execute_place_task(current_task)\n        elif current_task.action == 'detect_object':\n            self.execute_detection_task(current_task)\n        else:\n            self.get_logger().warn(f'Unknown task action: {current_task.action}')\n            self.task_completed()\n\n    def execute_navigation_task(self, task):\n        \"\"\"Execute navigation task\"\"\"\n        try:\n            # Parse target location from parameters\n            params = json.loads(task.parameters) if task.parameters else {}\n\n            if 'location' in params:\n                location = params['location']\n\n                # Create navigation goal\n                goal = NavigateToPose.Goal()\n                goal.pose.header.frame_id = 'map'\n                goal.pose.pose.position.x = float(location[0])\n                goal.pose.pose.position.y = float(location[1])\n                goal.pose.pose.position.z = float(location[2])\n                goal.pose.pose.orientation.w = 1.0  # Default orientation\n\n                # Send navigation goal\n                self.nav_client.wait_for_server()\n                future = self.nav_client.send_goal_async(goal)\n                future.add_done_callback(self.navigation_result_callback)\n\n                # Publish status\n                status_msg = String()\n                status_msg.data = f'navigating_to: [{location[0]}, {location[1]}, {location[2]}]'\n                self.status_pub.publish(status_msg)\n\n            else:\n                self.get_logger().warn('Navigation task missing location parameter')\n                self.task_completed()\n\n        except Exception as e:\n            self.get_logger().error(f'Error executing navigation task: {e}')\n            self.task_completed()\n\n    def navigation_result_callback(self, future):\n        \"\"\"Handle navigation result\"\"\"\n        try:\n            goal_handle = future.result()\n            if goal_handle.accepted:\n                self.get_logger().info('Navigation goal accepted')\n                # Wait for result\n                result_future = goal_handle.get_result_async()\n                result_future.add_done_callback(self.navigation_finished_callback)\n            else:\n                self.get_logger().warn('Navigation goal rejected')\n                self.task_completed()\n        except Exception as e:\n            self.get_logger().error(f'Navigation error: {e}')\n            self.task_completed()\n\n    def navigation_finished_callback(self, future):\n        \"\"\"Handle navigation completion\"\"\"\n        try:\n            result = future.result().result\n            self.get_logger().info(f'Navigation completed with result: {result}')\n        except Exception as e:\n            self.get_logger().error(f'Navigation result error: {e}')\n        finally:\n            self.task_completed()\n\n    def execute_grasp_task(self, task):\n        \"\"\"Execute grasp task (placeholder implementation)\"\"\"\n        try:\n            params = json.loads(task.parameters) if task.parameters else {}\n\n            if 'object' in params:\n                object_name = params['object']\n                self.get_logger().info(f'Attempting to grasp object: {object_name}')\n\n                # Publish status\n                status_msg = String()\n                status_msg.data = f'grasping_object: {object_name}'\n                self.status_pub.publish(status_msg)\n\n                # Simulate grasp operation completion\n                # In real implementation, you would use robot manipulation interfaces\n                self.get_logger().info('Grasp operation completed')\n\n            else:\n                self.get_logger().warn('Grasp task missing object parameter')\n\n        except Exception as e:\n            self.get_logger().error(f'Error executing grasp task: {e}')\n        finally:\n            self.task_completed()\n\n    def execute_place_task(self, task):\n        \"\"\"Execute place task (placeholder implementation)\"\"\"\n        try:\n            params = json.loads(task.parameters) if task.parameters else {}\n\n            if 'object' in params:\n                object_name = params['object']\n                self.get_logger().info(f'Attempting to place object: {object_name}')\n\n                # Publish status\n                status_msg = String()\n                status_msg.data = f'placing_object: {object_name}'\n                self.status_pub.publish(status_msg)\n\n                # Simulate place operation completion\n                self.get_logger().info('Place operation completed')\n\n            else:\n                self.get_logger().warn('Place task missing object parameter')\n\n        except Exception as e:\n            self.get_logger().error(f'Error executing place task: {e}')\n        finally:\n            self.task_completed()\n\n    def execute_detection_task(self, task):\n        \"\"\"Execute detection task (placeholder implementation)\"\"\"\n        try:\n            params = json.loads(task.parameters) if task.parameters else {}\n\n            if 'object' in params:\n                object_name = params['object']\n                self.get_logger().info(f'Attempting to detect object: {object_name}')\n\n                # Publish status\n                status_msg = String()\n                status_msg.data = f'detecting_object: {object_name}'\n                self.status_pub.publish(status_msg)\n\n                # Simulate detection operation completion\n                self.get_logger().info('Detection operation completed')\n\n            else:\n                self.get_logger().warn('Detection task missing object parameter')\n\n        except Exception as e:\n            self.get_logger().error(f'Error executing detection task: {e}')\n        finally:\n            self.task_completed()\n\n    def task_completed(self):\n        \"\"\"Mark current task as completed and move to next\"\"\"\n        with self.execution_lock:\n            self.current_task_index += 1\n            if self.current_task_index >= len(self.current_plan.tasks):\n                self.plan_completed()\n            else:\n                self.get_logger().info(f'Task completed. Moving to task {self.current_task_index + 1}')\n\n    def plan_completed(self):\n        \"\"\"Handle plan completion\"\"\"\n        with self.execution_lock:\n            if self.current_plan:\n                self.get_logger().info(f'Plan completed with {len(self.current_plan.tasks)} tasks')\n\n                # Publish completion status\n                status_msg = String()\n                status_msg.data = 'plan_completed'\n                self.status_pub.publish(status_msg)\n\n                self.current_plan = None\n                self.current_task_index = 0\n                self.is_executing = False\n\n    def cancel_current_plan(self):\n        \"\"\"Cancel the current plan execution\"\"\"\n        with self.execution_lock:\n            self.current_plan = None\n            self.current_task_index = 0\n            self.is_executing = False\n            self.get_logger().info('Current plan cancelled')\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VLAActionExecutorNode()\n\n    # Use multi-threaded executor to handle callbacks\n    executor = MultiThreadedExecutor()\n    executor.add_node(node)\n\n    try:\n        executor.spin()\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n        executor.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"whisper-integration-for-speech-to-text",children:"Whisper Integration for Speech-to-Text"}),"\n",(0,s.jsx)(n.h3,{id:"openai-whisper-node",children:"OpenAI Whisper Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom audio_common_msgs.msg import AudioData\nfrom sensor_msgs.msg import AudioData as SensorAudioData\nimport torch\nimport whisper\nimport numpy as np\nimport io\nimport wave\nimport threading\nimport queue\n\n\nclass WhisperSTTNode(Node):\n\n    def __init__(self):\n        super().__init__(\'whisper_stt_node\')\n\n        # Publishers and Subscribers\n        self.audio_sub = self.create_subscription(\n            SensorAudioData, \'audio\', self.audio_callback, 10)\n        self.command_pub = self.create_publisher(\n            String, \'voice_commands\', 10)\n\n        # Initialize Whisper model\n        self.get_logger().info(\'Loading Whisper model...\')\n        self.model = whisper.load_model("base")  # Use "tiny", "base", "small", "medium", or "large"\n        self.get_logger().info(\'Whisper model loaded\')\n\n        # Audio processing queue\n        self.audio_queue = queue.Queue()\n        self.processing_thread = threading.Thread(target=self.process_audio_queue)\n        self.processing_thread.daemon = True\n        self.processing_thread.start()\n\n        self.get_logger().info(\'Whisper STT Node initialized\')\n\n    def audio_callback(self, msg):\n        """Receive audio data and add to processing queue"""\n        try:\n            # Convert audio data to format expected by Whisper\n            audio_data = np.frombuffer(msg.data, dtype=np.int16)\n            audio_float = audio_data.astype(np.float32) / 32768.0  # Normalize to [-1, 1]\n\n            # Add to processing queue\n            self.audio_queue.put(audio_float)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing audio: {e}\')\n\n    def process_audio_queue(self):\n        """Process audio from queue using Whisper"""\n        while rclpy.ok():\n            try:\n                # Get audio from queue (with timeout to allow thread to exit)\n                audio_float = self.audio_queue.get(timeout=1.0)\n\n                # Process with Whisper\n                result = self.model.transcribe(audio_float)\n                text = result[\'text\'].strip()\n\n                if text:  # Only publish non-empty results\n                    self.get_logger().info(f\'Recognized: {text}\')\n\n                    # Publish recognized command\n                    cmd_msg = String()\n                    cmd_msg.data = text\n                    self.command_pub.publish(cmd_msg)\n\n            except queue.Empty:\n                # Timeout - continue loop\n                continue\n            except Exception as e:\n                self.get_logger().error(f\'Error in audio processing: {e}\')\n\n    def destroy_node(self):\n        """Clean up before node destruction"""\n        if self.processing_thread.is_alive():\n            self.processing_thread.join(timeout=2.0)  # Wait up to 2 seconds\n        super().destroy_node()\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = WhisperSTTNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"complete-vla-system-integration",children:"Complete VLA System Integration"}),"\n",(0,s.jsx)(n.h3,{id:"main-vla-system-node",children:"Main VLA System Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom vla_interfaces.msg import ParsedCommand, TaskPlan\nfrom geometry_msgs.msg import Twist\nimport threading\nimport time\n\n\nclass VLAMasterNode(Node):\n\n    def __init__(self):\n        super().__init__('vla_master_node')\n\n        # Publishers\n        self.status_pub = self.create_publisher(String, 'vla_status', 10)\n        self.cmd_vel_pub = self.create_publisher(Twist, 'cmd_vel', 10)\n\n        # Subscribers\n        self.command_sub = self.create_subscription(\n            String, 'voice_commands', self.voice_command_callback, 10)\n        self.parsed_sub = self.create_subscription(\n            ParsedCommand, 'parsed_commands', self.parsed_command_callback, 10)\n        self.plan_sub = self.create_subscription(\n            TaskPlan, 'task_plans', self.task_plan_callback, 10)\n\n        # System state\n        self.system_state = 'idle'  # idle, processing, executing\n        self.last_command = ''\n        self.active_plan = None\n\n        # Status reporting timer\n        self.status_timer = self.create_timer(5.0, self.report_status)\n\n        self.get_logger().info('VLA Master Node initialized')\n\n    def voice_command_callback(self, msg):\n        \"\"\"Handle incoming voice commands\"\"\"\n        command = msg.data\n        self.get_logger().info(f'Received voice command: {command}')\n\n        self.last_command = command\n        self.system_state = 'processing'\n\n        # Update status\n        status_msg = String()\n        status_msg.data = f'processing_command: {command}'\n        self.status_pub.publish(status_msg)\n\n    def parsed_command_callback(self, msg):\n        \"\"\"Handle parsed commands\"\"\"\n        self.get_logger().info(f'Parsed command: {msg.action} for object \"{msg.target_object}\" at location \"{msg.target_location}\"')\n\n        # Could trigger immediate action or planning based on complexity\n        if msg.action == 'report_status':\n            self.report_robot_status()\n\n    def task_plan_callback(self, msg):\n        \"\"\"Handle incoming task plans\"\"\"\n        self.get_logger().info(f'Received task plan with {len(msg.tasks)} tasks')\n        self.active_plan = msg\n        self.system_state = 'executing'\n\n        # Update status\n        status_msg = String()\n        status_msg.data = f'executing_plan_with_{len(msg.tasks)}_tasks'\n        self.status_pub.publish(status_msg)\n\n    def report_status(self):\n        \"\"\"Periodically report system status\"\"\"\n        status_msg = String()\n        status_msg.data = f'system_state: {self.system_state}, last_command: {self.last_command[:50] if self.last_command else \"none\"}'\n        self.status_pub.publish(status_msg)\n\n    def report_robot_status(self):\n        \"\"\"Report current robot status\"\"\"\n        # In a real implementation, this would gather actual robot status\n        status_msg = String()\n        status_msg.data = 'robot_status: battery_85%, position_known, systems_operational'\n        self.status_pub.publish(status_msg)\n\n    def emergency_stop(self):\n        \"\"\"Emergency stop function\"\"\"\n        twist = Twist()\n        twist.linear.x = 0.0\n        twist.linear.y = 0.0\n        twist.linear.z = 0.0\n        twist.angular.x = 0.0\n        twist.angular.y = 0.0\n        twist.angular.z = 0.0\n        self.cmd_vel_pub.publish(twist)\n        self.get_logger().warn('Emergency stop activated')\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VLAMasterNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Large Language Models (LLMs)"}),": AI models for natural language understanding"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Vision-Language Integration"}),": Combining visual and textual information"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Speech-to-Text (STT)"}),": Converting speech to text using systems like Whisper"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action Mapping"}),": Connecting language concepts to robot actions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Task Planning"}),": Decomposing high-level commands into executable steps"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ROS Actions"}),": Asynchronous goal-oriented communication with feedback"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Vision Grounding"}),": Connecting language to visual elements in the environment"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time Processing"}),": Handling inputs with low latency requirements"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"practical-exercises",children:"Practical Exercises"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Implement a complete VLA system with your preferred LLM"}),"\n",(0,s.jsx)(n.li,{children:"Integrate visual grounding with language understanding"}),"\n",(0,s.jsx)(n.li,{children:"Create a robust action mapping system for your robot platform"}),"\n",(0,s.jsx)(n.li,{children:"Implement error handling and recovery in the VLA pipeline"}),"\n",(0,s.jsx)(n.li,{children:"Add support for multi-modal inputs (speech, text, gestures)"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"common-failure-modes",children:"Common Failure Modes"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LLM Hallucination"}),": Language models generating incorrect or fabricated information"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Audio Quality Issues"}),": Poor audio affecting speech recognition accuracy"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Context Confusion"}),": LLMs losing track of conversation context"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Timing Issues"}),": Delays in processing affecting natural interaction"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Ambiguity Resolution"}),": Difficulty in resolving ambiguous commands"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Resource Exhaustion"}),": High computational requirements affecting performance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety Violations"}),": Actions executed without proper safety checks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Misinterpretation"}),": Natural language not correctly mapped to robot actions"]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>r,x:()=>i});var a=t(6540);const s={},o=a.createContext(s);function r(e){const n=a.useContext(o);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),a.createElement(o.Provider,{value:n},e.children)}}}]);