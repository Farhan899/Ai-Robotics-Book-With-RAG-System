"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[355],{7705(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>m,frontMatter:()=>s,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"capstone/hands-on","title":"Capstone Hands-On: Building the Autonomous Humanoid","description":"Overview","source":"@site/docs/capstone/hands-on.md","sourceDirName":"capstone","slug":"/capstone/hands-on","permalink":"/docs/capstone/hands-on","draft":false,"unlisted":false,"editUrl":"https://github.com/Farhan899/Ai-Robotics-Book/edit/main/docs/capstone/hands-on.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Capstone Architecture: Full System Architecture Diagram","permalink":"/docs/capstone/architecture"},"next":{"title":"Capstone Code Examples: Complete Integration Code","permalink":"/docs/capstone/code-examples"}}');var a=t(4848),i=t(8453);const s={sidebar_position:3},r="Capstone Hands-On: Building the Autonomous Humanoid",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Project Setup",id:"project-setup",level:2},{value:"1. Create the Project Structure",id:"1-create-the-project-structure",level:3},{value:"2. Environment Configuration",id:"2-environment-configuration",level:3},{value:"Voice Command Integration",id:"voice-command-integration",level:2},{value:"3. Implement Voice Command Processing Node",id:"3-implement-voice-command-processing-node",level:3},{value:"4. Create the Audio Capture Node",id:"4-create-the-audio-capture-node",level:3},{value:"5. Create Launch File for Voice System",id:"5-create-launch-file-for-voice-system",level:3},{value:"Perception System Integration",id:"perception-system-integration",level:2},{value:"6. Implement Perception Node with Isaac ROS",id:"6-implement-perception-node-with-isaac-ros",level:3},{value:"Navigation System Integration",id:"navigation-system-integration",level:2},{value:"7. Implement Navigation Node",id:"7-implement-navigation-node",level:3},{value:"Manipulation System Integration",id:"manipulation-system-integration",level:2},{value:"8. Implement Manipulation Node",id:"8-implement-manipulation-node",level:3},{value:"Task Planning Integration",id:"task-planning-integration",level:2},{value:"9. Implement Task Planning Node",id:"9-implement-task-planning-node",level:3},{value:"Simulation Integration",id:"simulation-integration",level:2},{value:"10. Create Gazebo Launch File",id:"10-create-gazebo-launch-file",level:3},{value:"11. Create Complete System Launch File",id:"11-create-complete-system-launch-file",level:3},{value:"Testing the Complete System",id:"testing-the-complete-system",level:2},{value:"12. Run the Complete System",id:"12-run-the-complete-system",level:3},{value:"13. Test Voice Commands",id:"13-test-voice-commands",level:3},{value:"14. Monitor System Status",id:"14-monitor-system-status",level:3},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:2},{value:"15. Common Issues and Solutions",id:"15-common-issues-and-solutions",level:3},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Practical Exercises",id:"practical-exercises",level:2},{value:"Common Failure Modes",id:"common-failure-modes",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"capstone-hands-on-building-the-autonomous-humanoid",children:"Capstone Hands-On: Building the Autonomous Humanoid"})}),"\n",(0,a.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(n.p,{children:"In this hands-on module, you'll implement the complete autonomous humanoid system by integrating all the components from the previous modules. This comprehensive exercise will demonstrate how to combine ROS 2 middleware, Gazebo simulation, NVIDIA Isaac perception, and Vision-Language-Action capabilities into a functioning voice-controlled robot."}),"\n",(0,a.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsx)(n.p,{children:"Before starting this hands-on exercise, ensure you have:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Completed all previous modules (Modules 1-4)"}),"\n",(0,a.jsx)(n.li,{children:"All required software installed (ROS 2 Humble, Gazebo, Isaac Sim, Python 3.8+)"}),"\n",(0,a.jsx)(n.li,{children:"OpenAI API key configured for LLM integration"}),"\n",(0,a.jsx)(n.li,{children:"Working knowledge of ROS 2 concepts (nodes, topics, services, actions)"}),"\n",(0,a.jsx)(n.li,{children:"Basic understanding of computer vision and perception systems"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"project-setup",children:"Project Setup"}),"\n",(0,a.jsx)(n.h3,{id:"1-create-the-project-structure",children:"1. Create the Project Structure"}),"\n",(0,a.jsx)(n.p,{children:"First, let's set up the workspace for our autonomous humanoid project:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Create the workspace\nmkdir -p ~/autonomous_humanoid_ws/src\ncd ~/autonomous_humanoid_ws\n\n# Create the package structure\ncd src\ngit clone https://github.com/your-organization/humanoid_description.git\ngit clone https://github.com/your-organization/humanoid_bringup.git\ngit clone https://github.com/your-organization/humanoid_voice_control.git\ngit clone https://github.com/your-organization/humanoid_perception.git\ngit clone https://github.com/your-organization/humanoid_navigation.git\ngit clone https://github.com/your-organization/humanoid_manipulation.git\ngit clone https://github.com/your-organization/humanoid_planning.git\ngit clone https://github.com/your-organization/humanoid_simulation.git\ngit clone https://github.com/your-organization/humanoid_msgs.git\n\n# Build the workspace\ncd ~/autonomous_humanoid_ws\ncolcon build --packages-select humanoid_msgs\nsource install/setup.bash\ncolcon build\nsource install/setup.bash\n"})}),"\n",(0,a.jsx)(n.h3,{id:"2-environment-configuration",children:"2. Environment Configuration"}),"\n",(0,a.jsx)(n.p,{children:"Set up the necessary environment variables:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'# Add to ~/.bashrc or ~/.zshrc\nexport OPENAI_API_KEY="your-openai-api-key"\nexport HUMANOID_ROBOT=1\nexport GAZEBO_MODEL_PATH=$GAZEBO_MODEL_PATH:~/autonomous_humanoid_ws/src/humanoid_description/models\nexport ROS_DOMAIN_ID=42\n'})}),"\n",(0,a.jsx)(n.h2,{id:"voice-command-integration",children:"Voice Command Integration"}),"\n",(0,a.jsx)(n.h3,{id:"3-implement-voice-command-processing-node",children:"3. Implement Voice Command Processing Node"}),"\n",(0,a.jsx)(n.p,{children:"Create the voice command processing node that will handle speech-to-text conversion:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n# File: humanoid_voice_control/humanoid_voice_control/voice_command_node.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom audio_common_msgs.msg import AudioData\nimport speech_recognition as sr\nimport openai\nimport json\nimport threading\nimport queue\nfrom humanoid_msgs.msg import ParsedCommand\nfrom humanoid_msgs.srv import CommandValidation\n\nclass VoiceCommandNode(Node):\n    def __init__(self):\n        super().__init__('voice_command_node')\n\n        # Initialize speech recognizer\n        self.recognizer = sr.Recognizer()\n        self.recognizer.energy_threshold = 4000\n        self.recognizer.dynamic_energy_threshold = True\n\n        # Audio queue for processing\n        self.audio_queue = queue.Queue()\n\n        # Publishers and subscribers\n        self.voice_cmd_pub = self.create_publisher(String, '/voice_commands', 10)\n        self.parsed_cmd_pub = self.create_publisher(ParsedCommand, '/parsed_commands', 10)\n        self.audio_sub = self.create_subscription(\n            AudioData, '/audio', self.audio_callback, 10)\n\n        # Service for command validation\n        self.validation_srv = self.create_service(\n            CommandValidation, '/validate_command', self.validate_command_callback)\n\n        # Timer for audio processing\n        self.process_timer = self.create_timer(0.1, self.process_audio)\n\n        # Initialize OpenAI\n        openai.api_key = self.get_parameter_or_set('openai_api_key', 'your-openai-key')\n\n        self.get_logger().info('Voice Command Node initialized')\n\n    def get_parameter_or_set(self, name, default_value):\n        self.declare_parameter(name, default_value)\n        return self.get_parameter(name).get_parameter_value().string_value\n\n    def audio_callback(self, msg):\n        \"\"\"Callback for audio data\"\"\"\n        self.audio_queue.put(msg)\n\n    def process_audio(self):\n        \"\"\"Process audio from queue\"\"\"\n        try:\n            audio_data = self.audio_queue.get_nowait()\n            self.process_audio_data(audio_data)\n        except queue.Empty:\n            pass\n\n    def process_audio_data(self, audio_data):\n        \"\"\"Process raw audio data to text\"\"\"\n        try:\n            # Convert audio data to audio file format\n            with sr.AudioData(audio_data.data, 16000, 2) as source:\n                # Perform speech recognition\n                text = self.recognizer.recognize_google(source)\n\n                if text:\n                    # Publish the recognized text\n                    cmd_msg = String()\n                    cmd_msg.data = text\n                    self.voice_cmd_pub.publish(cmd_msg)\n\n                    # Parse and publish structured command\n                    parsed_cmd = self.parse_command_with_llm(text)\n                    if parsed_cmd:\n                        self.parsed_cmd_pub.publish(parsed_cmd)\n\n        except sr.UnknownValueError:\n            self.get_logger().warn('Could not understand audio')\n        except sr.RequestError as e:\n            self.get_logger().error(f'Could not request results from speech service; {e}')\n        except Exception as e:\n            self.get_logger().error(f'Error processing audio: {e}')\n\n    def parse_command_with_llm(self, text):\n        \"\"\"Use LLM to parse natural language command into structured format\"\"\"\n        try:\n            response = openai.ChatCompletion.create(\n                model=\"gpt-3.5-turbo\",\n                messages=[\n                    {\"role\": \"system\", \"content\": \"\"\"You are a command parser for a humanoid robot.\n                    Parse the user's natural language command into a structured format.\n                    Return JSON with action_type, target_object, target_location, and parameters.\n                    Example: {'action_type': 'navigation', 'target_location': 'kitchen',\n                    'target_object': null, 'parameters': {}}\"\"\"},\n                    {\"role\": \"user\", \"content\": text}\n                ],\n                temperature=0.1\n            )\n\n            # Parse the response\n            result = json.loads(response.choices[0].message.content)\n\n            # Create parsed command message\n            parsed_cmd = ParsedCommand()\n            parsed_cmd.action_type = result.get('action_type', '')\n            parsed_cmd.target_object = result.get('target_object', '')\n            parsed_cmd.target_location = result.get('target_location', '')\n            parsed_cmd.parameters = json.dumps(result.get('parameters', {}))\n            parsed_cmd.original_command = text\n\n            return parsed_cmd\n\n        except Exception as e:\n            self.get_logger().error(f'Error parsing command with LLM: {e}')\n            return None\n\n    def validate_command_callback(self, request, response):\n        \"\"\"Validate if a command is safe to execute\"\"\"\n        # Basic validation logic\n        dangerous_keywords = ['harm', 'damage', 'break', 'destroy']\n\n        is_safe = not any(keyword in request.command.lower() for keyword in dangerous_keywords)\n        response.is_valid = is_safe\n        response.reason = \"Command is safe\" if is_safe else \"Command contains potentially unsafe keywords\"\n\n        return response\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VoiceCommandNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info('Shutting down voice command node')\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h3,{id:"4-create-the-audio-capture-node",children:"4. Create the Audio Capture Node"}),"\n",(0,a.jsx)(n.p,{children:"Create a node to capture audio from the microphone:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n# File: humanoid_voice_control/humanoid_voice_control/audio_capture_node.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom audio_common_msgs.msg import AudioData\nimport pyaudio\nimport threading\n\nclass AudioCaptureNode(Node):\n    def __init__(self):\n        super().__init__('audio_capture_node')\n\n        # Publisher for audio data\n        self.audio_pub = self.create_publisher(AudioData, '/audio', 10)\n\n        # Audio parameters\n        self.rate = 16000\n        self.chunk = 1024\n        self.format = pyaudio.paInt16\n        self.channels = 1\n\n        # Initialize PyAudio\n        self.pyaudio = pyaudio.PyAudio()\n\n        # Start audio capture thread\n        self.capture_thread = threading.Thread(target=self.capture_audio)\n        self.capture_thread.daemon = True\n        self.capture_thread.start()\n\n        self.get_logger().info('Audio Capture Node initialized')\n\n    def capture_audio(self):\n        \"\"\"Capture audio from microphone\"\"\"\n        try:\n            stream = self.pyaudio.open(\n                format=self.format,\n                channels=self.channels,\n                rate=self.rate,\n                input=True,\n                frames_per_buffer=self.chunk\n            )\n\n            self.get_logger().info('Audio capture started')\n\n            while rclpy.ok():\n                data = stream.read(self.chunk, exception_on_overflow=False)\n\n                # Create and publish audio message\n                audio_msg = AudioData()\n                audio_msg.data = data\n                self.audio_pub.publish(audio_msg)\n\n        except Exception as e:\n            self.get_logger().error(f'Error in audio capture: {e}')\n        finally:\n            if 'stream' in locals():\n                stream.stop_stream()\n                stream.close()\n            self.pyaudio.terminate()\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = AudioCaptureNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info('Shutting down audio capture node')\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h3,{id:"5-create-launch-file-for-voice-system",children:"5. Create Launch File for Voice System"}),"\n",(0,a.jsx)(n.p,{children:"Create a launch file to start the voice command system:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:"\x3c!-- File: humanoid_voice_control/launch/voice_system.launch.py --\x3e\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    return LaunchDescription([\n        Node(\n            package='humanoid_voice_control',\n            executable='audio_capture_node',\n            name='audio_capture',\n            output='screen'\n        ),\n        Node(\n            package='humanoid_voice_control',\n            executable='voice_command_node',\n            name='voice_command',\n            output='screen',\n            parameters=[\n                {'openai_api_key': os.environ.get('OPENAI_API_KEY', 'your-key-here')}\n            ]\n        )\n    ])\n"})}),"\n",(0,a.jsx)(n.h2,{id:"perception-system-integration",children:"Perception System Integration"}),"\n",(0,a.jsx)(n.h3,{id:"6-implement-perception-node-with-isaac-ros",children:"6. Implement Perception Node with Isaac ROS"}),"\n",(0,a.jsx)(n.p,{children:"Create a perception node that integrates Isaac ROS for object detection:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n# File: humanoid_perception/humanoid_perception/perception_node.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom vision_msgs.msg import Detection2DArray, ObjectHypothesisWithPose\nfrom geometry_msgs.msg import Point\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nfrom humanoid_msgs.msg import DetectedObject, SemanticScene\n\nclass PerceptionNode(Node):\n    def __init__(self):\n        super().__init__('perception_node')\n\n        # Initialize CV bridge\n        self.bridge = CvBridge()\n\n        # Publishers and subscribers\n        self.image_sub = self.create_subscription(\n            Image, '/camera/image_raw', self.image_callback, 10)\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo, '/camera/camera_info', self.camera_info_callback, 10)\n        self.detection_pub = self.create_publisher(\n            Detection2DArray, '/detected_objects', 10)\n        self.semantic_pub = self.create_publisher(\n            SemanticScene, '/semantic_scene', 10)\n\n        # Camera parameters\n        self.camera_matrix = None\n        self.dist_coeffs = None\n\n        # Object detection parameters\n        self.confidence_threshold = 0.5\n\n        self.get_logger().info('Perception Node initialized')\n\n    def camera_info_callback(self, msg):\n        \"\"\"Callback for camera info\"\"\"\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\n        self.dist_coeffs = np.array(msg.d)\n\n    def image_callback(self, msg):\n        \"\"\"Callback for image data\"\"\"\n        try:\n            # Convert ROS image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n            # Perform object detection\n            detections = self.detect_objects(cv_image)\n\n            # Publish detections\n            detection_msg = self.create_detection_message(detections, msg.header)\n            self.detection_pub.publish(detection_msg)\n\n            # Create and publish semantic scene\n            semantic_msg = self.create_semantic_scene(detections, msg.header)\n            self.semantic_pub.publish(semantic_msg)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n\n    def detect_objects(self, image):\n        \"\"\"Perform object detection on image\"\"\"\n        # This is a simplified example - in practice, you'd use Isaac ROS DNN nodes\n        # or integrate with a real object detection model\n        detections = []\n\n        # Example: Use a pre-trained model (YOLO, etc.)\n        # For this example, we'll use a simple color-based detection\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n        # Detect red objects (example)\n        lower_red = np.array([0, 100, 100])\n        upper_red = np.array([10, 255, 255])\n        mask_red1 = cv2.inRange(hsv, lower_red, upper_red)\n\n        lower_red = np.array([170, 100, 100])\n        upper_red = np.array([180, 255, 255])\n        mask_red2 = cv2.inRange(hsv, lower_red, upper_red)\n\n        mask_red = mask_red1 + mask_red2\n\n        # Find contours\n        contours, _ = cv2.findContours(mask_red, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        for contour in contours:\n            area = cv2.contourArea(contour)\n            if area > 1000:  # Filter small areas\n                x, y, w, h = cv2.boundingRect(contour)\n\n                detection = {\n                    'class': 'red_object',\n                    'confidence': 0.8,\n                    'bbox': [x, y, w, h],\n                    'center': [x + w//2, y + h//2]\n                }\n                detections.append(detection)\n\n        return detections\n\n    def create_detection_message(self, detections, header):\n        \"\"\"Create vision_msgs/Detection2DArray message\"\"\"\n        detection_array = Detection2DArray()\n        detection_array.header = header\n\n        for detection in detections:\n            if detection['confidence'] > self.confidence_threshold:\n                vision_detection = Detection2D()\n\n                # Set bounding box\n                vision_detection.bbox.center.x = detection['center'][0]\n                vision_detection.bbox.center.y = detection['center'][1]\n                vision_detection.bbox.size_x = detection['bbox'][2]\n                vision_detection.bbox.size_y = detection['bbox'][3]\n\n                # Set results\n                result = ObjectHypothesisWithPose()\n                result.hypothesis.class_id = detection['class']\n                result.hypothesis.score = detection['confidence']\n                vision_detection.results.append(result)\n\n                detection_array.detections.append(vision_detection)\n\n        return detection_array\n\n    def create_semantic_scene(self, detections, header):\n        \"\"\"Create semantic scene message\"\"\"\n        semantic_scene = SemanticScene()\n        semantic_scene.header = header\n\n        for detection in detections:\n            if detection['confidence'] > self.confidence_threshold:\n                obj = DetectedObject()\n                obj.name = detection['class']\n                obj.confidence = detection['confidence']\n                obj.position.x = detection['center'][0]\n                obj.position.y = detection['center'][1]\n                # Convert to 3D position if camera matrix is available\n                if self.camera_matrix is not None:\n                    # Simple depth assumption for example\n                    obj.position.z = 1.0\n\n                semantic_scene.objects.append(obj)\n\n        return semantic_scene\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = PerceptionNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info('Shutting down perception node')\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"navigation-system-integration",children:"Navigation System Integration"}),"\n",(0,a.jsx)(n.h3,{id:"7-implement-navigation-node",children:"7. Implement Navigation Node"}),"\n",(0,a.jsx)(n.p,{children:"Create a navigation node that integrates with Nav2:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n# File: humanoid_navigation/humanoid_navigation/navigation_node.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped, Point\nfrom humanoid_msgs.msg import ParsedCommand, NavigationGoal\nfrom humanoid_msgs.srv import NavigateToPose\nfrom nav2_msgs.action import NavigateToPose as NavigateToPoseAction\nfrom rclpy.action import ActionClient\nimport json\n\nclass NavigationNode(Node):\n    def __init__(self):\n        super().__init__('navigation_node')\n\n        # Action client for Nav2\n        self.nav_client = ActionClient(self, NavigateToPoseAction, 'navigate_to_pose')\n\n        # Publishers and subscribers\n        self.command_sub = self.create_subscription(\n            ParsedCommand, '/parsed_commands', self.command_callback, 10)\n        self.nav_goal_pub = self.create_publisher(\n            NavigationGoal, '/navigation_goals', 10)\n\n        # Service for navigation\n        self.nav_srv = self.create_service(\n            NavigateToPose, '/navigate_to_pose', self.navigate_to_pose_callback)\n\n        # Known locations mapping\n        self.location_map = {\n            'kitchen': Point(x=2.0, y=1.0, z=0.0),\n            'living_room': Point(x=-1.0, y=0.0, z=0.0),\n            'bedroom': Point(x=0.0, y=-2.0, z=0.0),\n            'office': Point(x=1.5, y=-1.0, z=0.0),\n            'entrance': Point(x=0.0, y=2.0, z=0.0)\n        }\n\n        self.get_logger().info('Navigation Node initialized')\n\n    def command_callback(self, msg):\n        \"\"\"Callback for parsed commands\"\"\"\n        if msg.action_type == 'navigation':\n            self.handle_navigation_command(msg)\n\n    def handle_navigation_command(self, cmd_msg):\n        \"\"\"Handle navigation commands\"\"\"\n        target_location = cmd_msg.target_location.lower()\n\n        if target_location in self.location_map:\n            target_point = self.location_map[target_location]\n\n            # Create navigation goal\n            goal_msg = NavigateToPoseAction.Goal()\n            goal_msg.pose.header.frame_id = 'map'\n            goal_msg.pose.pose.position = target_point\n            goal_msg.pose.pose.orientation.w = 1.0  # No rotation\n\n            # Send navigation goal\n            self.send_navigation_goal(goal_msg)\n\n            # Publish navigation goal message\n            nav_goal_msg = NavigationGoal()\n            nav_goal_msg.target_location = target_location\n            nav_goal_msg.target_pose = goal_msg.pose\n            self.nav_goal_pub.publish(nav_goal_msg)\n        else:\n            self.get_logger().warn(f'Unknown location: {target_location}')\n\n    def send_navigation_goal(self, goal_msg):\n        \"\"\"Send navigation goal to Nav2\"\"\"\n        self.nav_client.wait_for_server()\n\n        send_goal_future = self.nav_client.send_goal_async(goal_msg)\n        send_goal_future.add_done_callback(self.goal_response_callback)\n\n    def goal_response_callback(self, future):\n        \"\"\"Handle goal response\"\"\"\n        goal_handle = future.result()\n        if not goal_handle.accepted:\n            self.get_logger().info('Goal rejected')\n            return\n\n        self.get_logger().info('Goal accepted')\n        get_result_future = goal_handle.get_result_async()\n        get_result_future.add_done_callback(self.get_result_callback)\n\n    def get_result_callback(self, future):\n        \"\"\"Handle navigation result\"\"\"\n        result = future.result().result\n        self.get_logger().info(f'Navigation result: {result}')\n\n    def navigate_to_pose_callback(self, request, response):\n        \"\"\"Service callback for navigation\"\"\"\n        goal_msg = NavigateToPoseAction.Goal()\n        goal_msg.pose = request.target_pose\n\n        self.nav_client.wait_for_server()\n        send_goal_future = self.nav_client.send_goal_async(goal_msg)\n\n        # For simplicity, return immediately\n        # In a real system, you'd wait for completion or implement async handling\n        response.success = True\n        response.message = \"Navigation goal sent\"\n\n        return response\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = NavigationNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info('Shutting down navigation node')\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"manipulation-system-integration",children:"Manipulation System Integration"}),"\n",(0,a.jsx)(n.h3,{id:"8-implement-manipulation-node",children:"8. Implement Manipulation Node"}),"\n",(0,a.jsx)(n.p,{children:"Create a manipulation node for object interaction:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# File: humanoid_manipulation/humanoid_manipulation/manipulation_node.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom humanoid_msgs.msg import ParsedCommand, DetectedObject\nfrom humanoid_msgs.srv import ManipulateObject\nfrom geometry_msgs.msg import Pose, Point\nimport json\n\nclass ManipulationNode(Node):\n    def __init__(self):\n        super().__init__(\'manipulation_node\')\n\n        # Publishers and subscribers\n        self.command_sub = self.create_subscription(\n            ParsedCommand, \'/parsed_commands\', self.command_callback, 10)\n        self.objects_sub = self.create_subscription(\n            DetectedObject, \'/detected_objects\', self.objects_callback, 10)\n\n        # Service for manipulation\n        self.manip_srv = self.create_service(\n            ManipulateObject, \'/manipulate_object\', self.manipulate_callback)\n\n        # Detected objects storage\n        self.detected_objects = []\n\n        self.get_logger().info(\'Manipulation Node initialized\')\n\n    def command_callback(self, msg):\n        """Callback for parsed commands"""\n        if msg.action_type == \'manipulation\':\n            self.handle_manipulation_command(msg)\n\n    def objects_callback(self, msg):\n        """Callback for detected objects"""\n        self.detected_objects = msg.objects\n\n    def handle_manipulation_command(self, cmd_msg):\n        """Handle manipulation commands"""\n        target_object = cmd_msg.target_object.lower()\n\n        # Find the object in detected objects\n        object_to_manipulate = None\n        for obj in self.detected_objects:\n            if obj.name.lower() == target_object:\n                object_to_manipulate = obj\n                break\n\n        if object_to_manipulate:\n            # Perform manipulation (simplified for example)\n            self.get_logger().info(f\'Attempting to manipulate {target_object}\')\n\n            # In a real system, you would:\n            # 1. Plan grasp trajectory\n            # 2. Move to object position\n            # 3. Execute grasp\n            # 4. Verify success\n        else:\n            self.get_logger().warn(f\'Object {target_object} not found in scene\')\n\n    def manipulate_callback(self, request, response):\n        """Service callback for manipulation"""\n        # Simplified manipulation logic\n        try:\n            # In a real system, you would implement the full manipulation pipeline\n            if request.action == \'pick\':\n                response.success = self.execute_pick(request.object_name)\n            elif request.action == \'place\':\n                response.success = self.execute_place(request.location)\n            elif request.action == \'move\':\n                response.success = self.execute_move(request.object_name, request.target_pose)\n            else:\n                response.success = False\n                response.message = f\'Unknown action: {request.action}\'\n\n        except Exception as e:\n            response.success = False\n            response.message = f\'Error during manipulation: {str(e)}\'\n\n        if response.success:\n            response.message = f\'Successfully executed {request.action} action\'\n        else:\n            response.message = f\'Failed to execute {request.action} action\'\n\n        return response\n\n    def execute_pick(self, object_name):\n        """Execute pick action"""\n        # Simplified pick logic\n        self.get_logger().info(f\'Executing pick for {object_name}\')\n        # In a real system: plan grasp, move arm, close gripper, verify grasp\n        return True\n\n    def execute_place(self, location):\n        """Execute place action"""\n        # Simplified place logic\n        self.get_logger().info(f\'Executing place at {location}\')\n        # In a real system: plan placement, move to location, open gripper, verify placement\n        return True\n\n    def execute_move(self, object_name, target_pose):\n        """Execute move action"""\n        # Simplified move logic\n        self.get_logger().info(f\'Executing move for {object_name} to {target_pose}\')\n        # In a real system: pick up object, move to target pose, place object\n        return True\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ManipulationNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info(\'Shutting down manipulation node\')\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(n.h2,{id:"task-planning-integration",children:"Task Planning Integration"}),"\n",(0,a.jsx)(n.h3,{id:"9-implement-task-planning-node",children:"9. Implement Task Planning Node"}),"\n",(0,a.jsx)(n.p,{children:"Create a task planning node that coordinates all subsystems:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n# File: humanoid_planning/humanoid_planning/task_planning_node.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom humanoid_msgs.msg import ParsedCommand, TaskPlan, TaskStep\nfrom humanoid_msgs.srv import CommandValidation\nfrom std_msgs.msg import String\nimport json\n\nclass TaskPlanningNode(Node):\n    def __init__(self):\n        super().__init__('task_planning_node')\n\n        # Publishers and subscribers\n        self.command_sub = self.create_subscription(\n            ParsedCommand, '/parsed_commands', self.command_callback, 10)\n        self.plan_pub = self.create_publisher(TaskPlan, '/task_plans', 10)\n\n        # Service clients\n        self.validation_client = self.create_client(\n            CommandValidation, '/validate_command')\n\n        # Task execution state\n        self.current_task = None\n        self.task_queue = []\n\n        self.get_logger().info('Task Planning Node initialized')\n\n    def command_callback(self, msg):\n        \"\"\"Callback for parsed commands\"\"\"\n        # Validate the command first\n        if self.validate_command(msg.original_command):\n            # Plan the task\n            task_plan = self.create_task_plan(msg)\n            if task_plan:\n                self.plan_pub.publish(task_plan)\n                self.execute_task_plan(task_plan)\n\n    def validate_command(self, command):\n        \"\"\"Validate command using service\"\"\"\n        while not self.validation_client.wait_for_service(timeout_sec=1.0):\n            self.get_logger().info('Validation service not available, waiting...')\n\n        request = CommandValidation.Request()\n        request.command = command\n\n        future = self.validation_client.call_async(request)\n        rclpy.spin_until_future_complete(self, future)\n\n        response = future.result()\n        if response.is_valid:\n            self.get_logger().info(f'Command validated: {response.reason}')\n            return True\n        else:\n            self.get_logger().warn(f'Command validation failed: {response.reason}')\n            return False\n\n    def create_task_plan(self, parsed_cmd):\n        \"\"\"Create a task plan from parsed command\"\"\"\n        task_plan = TaskPlan()\n        task_plan.header.stamp = self.get_clock().now().to_msg()\n        task_plan.header.frame_id = 'map'\n        task_plan.original_command = parsed_cmd.original_command\n\n        # Based on action type, create appropriate task steps\n        if parsed_cmd.action_type == 'navigation':\n            # Navigation task\n            nav_step = TaskStep()\n            nav_step.action_type = 'navigation'\n            nav_step.target_location = parsed_cmd.target_location\n            nav_step.parameters = parsed_cmd.parameters\n            task_plan.steps.append(nav_step)\n\n        elif parsed_cmd.action_type == 'manipulation':\n            # Manipulation task - might need navigation first\n            if parsed_cmd.target_location:\n                # Navigate to location first\n                nav_step = TaskStep()\n                nav_step.action_type = 'navigation'\n                nav_step.target_location = parsed_cmd.target_location\n                nav_step.parameters = parsed_cmd.parameters\n                task_plan.steps.append(nav_step)\n\n            # Then manipulate object\n            manip_step = TaskStep()\n            manip_step.action_type = 'manipulation'\n            manip_step.target_object = parsed_cmd.target_object\n            manip_step.parameters = parsed_cmd.parameters\n            task_plan.steps.append(manip_step)\n\n        elif parsed_cmd.action_type == 'combined':\n            # Combined task - navigate and manipulate\n            nav_step = TaskStep()\n            nav_step.action_type = 'navigation'\n            nav_step.target_location = parsed_cmd.target_location\n            task_plan.steps.append(nav_step)\n\n            manip_step = TaskStep()\n            manip_step.action_type = 'manipulation'\n            manip_step.target_object = parsed_cmd.target_object\n            task_plan.steps.append(manip_step)\n\n        else:\n            self.get_logger().warn(f'Unknown action type: {parsed_cmd.action_type}')\n            return None\n\n        return task_plan\n\n    def execute_task_plan(self, task_plan):\n        \"\"\"Execute the task plan\"\"\"\n        self.get_logger().info(f'Executing task plan with {len(task_plan.steps)} steps')\n\n        for i, step in enumerate(task_plan.steps):\n            self.get_logger().info(f'Executing step {i+1}/{len(task_plan.steps)}: {step.action_type}')\n\n            # In a real system, you would:\n            # 1. Send the step to appropriate action server\n            # 2. Wait for completion\n            # 3. Handle errors\n            # 4. Update system state\n\n            # For this example, we'll just log the step\n            self.execute_task_step(step)\n\n        self.get_logger().info('Task plan execution completed')\n\n    def execute_task_step(self, step):\n        \"\"\"Execute a single task step\"\"\"\n        # This would interface with the appropriate subsystem\n        if step.action_type == 'navigation':\n            self.get_logger().info(f'Navigating to {step.target_location}')\n        elif step.action_type == 'manipulation':\n            self.get_logger().info(f'Manipulating {step.target_object}')\n        else:\n            self.get_logger().warn(f'Unknown step type: {step.action_type}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = TaskPlanningNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info('Shutting down task planning node')\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"simulation-integration",children:"Simulation Integration"}),"\n",(0,a.jsx)(n.h3,{id:"10-create-gazebo-launch-file",children:"10. Create Gazebo Launch File"}),"\n",(0,a.jsx)(n.p,{children:"Create a launch file to start the complete simulation:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:"\x3c!-- File: humanoid_simulation/launch/complete_simulation.launch.py --\x3e\nfrom launch import LaunchDescription\nfrom launch.actions import IncludeLaunchDescription\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    # Get package directories\n    gazebo_ros_package_dir = get_package_share_directory('gazebo_ros')\n    humanoid_description_dir = get_package_share_directory('humanoid_description')\n\n    return LaunchDescription([\n        # Start Gazebo server\n        IncludeLaunchDescription(\n            PythonLaunchDescriptionSource(\n                os.path.join(gazebo_ros_package_dir, 'launch', 'gzserver.launch.py')\n            )\n        ),\n\n        # Start Gazebo client\n        IncludeLaunchDescription(\n            PythonLaunchDescriptionSource(\n                os.path.join(gazebo_ros_package_dir, 'launch', 'gzclient.launch.py')\n            )\n        ),\n\n        # Spawn the humanoid robot in Gazebo\n        Node(\n            package='gazebo_ros',\n            executable='spawn_entity.py',\n            arguments=[\n                '-entity', 'humanoid_robot',\n                '-topic', 'robot_description',\n                '-x', '0.0',\n                '-y', '0.0',\n                '-z', '0.0'\n            ],\n            output='screen'\n        ),\n\n        # Robot state publisher\n        Node(\n            package='robot_state_publisher',\n            executable='robot_state_publisher',\n            output='screen',\n            parameters=[\n                {'use_sim_time': True},\n                {'robot_description':\n                    open(os.path.join(humanoid_description_dir, 'urdf', 'humanoid.urdf')).read()}\n            ]\n        ),\n\n        # Joint state publisher\n        Node(\n            package='joint_state_publisher',\n            executable='joint_state_publisher',\n            output='screen',\n            parameters=[\n                {'use_sim_time': True}\n            ]\n        )\n    ])\n"})}),"\n",(0,a.jsx)(n.h3,{id:"11-create-complete-system-launch-file",children:"11. Create Complete System Launch File"}),"\n",(0,a.jsx)(n.p,{children:"Create a launch file to start the entire autonomous humanoid system:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:"\x3c!-- File: humanoid_bringup/launch/autonomous_humanoid.launch.py --\x3e\nfrom launch import LaunchDescription\nfrom launch.actions import IncludeLaunchDescription\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    # Get package directories\n    humanoid_voice_dir = get_package_share_directory('humanoid_voice_control')\n    humanoid_simulation_dir = get_package_share_directory('humanoid_simulation')\n\n    return LaunchDescription([\n        # Start the simulation\n        IncludeLaunchDescription(\n            PythonLaunchDescriptionSource(\n                os.path.join(humanoid_simulation_dir, 'launch', 'complete_simulation.launch.py')\n            )\n        ),\n\n        # Start voice control system\n        IncludeLaunchDescription(\n            PythonLaunchDescriptionSource(\n                os.path.join(humanoid_voice_dir, 'launch', 'voice_system.launch.py')\n            )\n        ),\n\n        # Start perception system\n        Node(\n            package='humanoid_perception',\n            executable='perception_node',\n            name='perception_node',\n            output='screen'\n        ),\n\n        # Start navigation system\n        Node(\n            package='humanoid_navigation',\n            executable='navigation_node',\n            name='navigation_node',\n            output='screen'\n        ),\n\n        # Start manipulation system\n        Node(\n            package='humanoid_manipulation',\n            executable='manipulation_node',\n            name='manipulation_node',\n            output='screen'\n        ),\n\n        # Start task planning system\n        Node(\n            package='humanoid_planning',\n            executable='task_planning_node',\n            name='task_planning_node',\n            output='screen'\n        )\n    ])\n"})}),"\n",(0,a.jsx)(n.h2,{id:"testing-the-complete-system",children:"Testing the Complete System"}),"\n",(0,a.jsx)(n.h3,{id:"12-run-the-complete-system",children:"12. Run the Complete System"}),"\n",(0,a.jsx)(n.p,{children:"Now let's test the complete system by launching everything:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Source the workspace\ncd ~/autonomous_humanoid_ws\nsource install/setup.bash\n\n# Launch the complete autonomous humanoid system\nros2 launch humanoid_bringup autonomous_humanoid.launch.py\n"})}),"\n",(0,a.jsx)(n.h3,{id:"13-test-voice-commands",children:"13. Test Voice Commands"}),"\n",(0,a.jsx)(n.p,{children:"Once the system is running, you can test voice commands. In a separate terminal:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Test navigation commands\nros2 topic pub /voice_commands std_msgs/String \"data: 'Go to the kitchen'\"\n\n# Test manipulation commands\nros2 topic pub /voice_commands std_msgs/String \"data: 'Pick up the red cup'\"\n\n# Test combined commands\nros2 topic pub /voice_commands std_msgs/String \"data: 'Go to the living room and find the blue ball'\"\n"})}),"\n",(0,a.jsx)(n.h3,{id:"14-monitor-system-status",children:"14. Monitor System Status"}),"\n",(0,a.jsx)(n.p,{children:"Monitor the system to see how different components interact:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Monitor voice commands\nros2 topic echo /voice_commands\n\n# Monitor parsed commands\nros2 topic echo /parsed_commands\n\n# Monitor task plans\nros2 topic echo /task_plans\n\n# Monitor detected objects\nros2 topic echo /detected_objects\n\n# Check system services\nros2 service list\n\n# Check system actions\nros2 action list\n"})}),"\n",(0,a.jsx)(n.h2,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),"\n",(0,a.jsx)(n.h3,{id:"15-common-issues-and-solutions",children:"15. Common Issues and Solutions"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Audio not being captured"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Check microphone permissions"}),"\n",(0,a.jsx)(n.li,{children:"Verify audio device is selected as default"}),"\n",(0,a.jsxs)(n.li,{children:["Test with ",(0,a.jsx)(n.code,{children:"arecord -d 5 test.wav"})]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"LLM integration errors"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Verify OpenAI API key is set in environment"}),"\n",(0,a.jsx)(n.li,{children:"Check network connectivity"}),"\n",(0,a.jsx)(n.li,{children:"Confirm billing is active for your OpenAI account"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Navigation failures"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Check if Nav2 is properly installed"}),"\n",(0,a.jsx)(n.li,{children:"Verify map and localization are working"}),"\n",(0,a.jsx)(n.li,{children:"Ensure robot has proper transforms (tf)"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Object detection not working"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Check camera is publishing images"}),"\n",(0,a.jsx)(n.li,{children:"Verify camera calibration"}),"\n",(0,a.jsx)(n.li,{children:"Ensure perception node is running"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"System Integration"}),": All components must work together seamlessly"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Real-time Processing"}),": System must respond to commands within acceptable timeframes"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Component Coordination"}),": Different subsystems need to communicate effectively"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Error Handling"}),": Robust error handling across all components"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Safety Considerations"}),": Validation of commands before execution"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Modular Design"}),": Each component should be independently testable"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"practical-exercises",children:"Practical Exercises"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Extend the voice command vocabulary"}),': Add support for new commands like "turn left", "turn right", "stop", etc.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Improve object detection"}),": Integrate a real object detection model (YOLO, SSD, etc.) instead of the simple color-based detection."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Add more locations"}),": Expand the location map with more destinations in the simulated environment."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Implement error recovery"}),": Add mechanisms to handle and recover from failed navigation or manipulation attempts."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Add safety checks"}),": Implement more sophisticated safety validation before executing commands."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Create a GUI interface"}),": Build a simple GUI that displays system status and allows manual command input."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"common-failure-modes",children:"Common Failure Modes"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Integration Failures"}),": Components not properly communicating with each other"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Timing Issues"}),": Delays causing poor user experience or system instability"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Resource Exhaustion"}),": High computational requirements affecting performance"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Safety Violations"}),": Actions executed without proper safety checks"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Error Propagation"}),": Failures in one component affecting others"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Communication Failures"}),": ROS topics/services not connecting properly"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Perception Failures"}),": Object detection or scene understanding errors"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Action Execution Failures"}),": Navigation or manipulation errors"]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>s,x:()=>r});var o=t(6540);const a={},i=o.createContext(a);function s(e){const n=o.useContext(i);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);