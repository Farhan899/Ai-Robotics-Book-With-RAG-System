"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[952],{86(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>s,metadata:()=>a,toc:()=>m});const a=JSON.parse('{"id":"module-3-ai-robot-brain/code-examples","title":"Code Examples: Isaac ROS Pipelines and AI Perception","description":"Isaac ROS VSLAM Pipeline","source":"@site/docs/module-3-ai-robot-brain/code-examples.md","sourceDirName":"module-3-ai-robot-brain","slug":"/module-3-ai-robot-brain/code-examples","permalink":"/docs/module-3-ai-robot-brain/code-examples","draft":false,"unlisted":false,"editUrl":"https://github.com/Farhan899/Ai-Robotics-Book/edit/main/docs/module-3-ai-robot-brain/code-examples.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Hands-On: NVIDIA Isaac AI Perception and Training","permalink":"/docs/module-3-ai-robot-brain/hands-on"},"next":{"title":"Troubleshooting: AI Perception and Isaac Common Issues","permalink":"/docs/module-3-ai-robot-brain/troubleshooting"}}');var t=i(4848),r=i(8453);const s={},o="Code Examples: Isaac ROS Pipelines and AI Perception",l={},m=[{value:"Isaac ROS VSLAM Pipeline",id:"isaac-ros-vslam-pipeline",level:2},{value:"Basic VSLAM Node Implementation",id:"basic-vslam-node-implementation",level:3},{value:"Isaac ROS Stereo DNN Pipeline",id:"isaac-ros-stereo-dnn-pipeline",level:2},{value:"Stereo DNN Node Implementation",id:"stereo-dnn-node-implementation",level:3},{value:"Synthetic Data Generation Script",id:"synthetic-data-generation-script",level:2},{value:"Domain Randomization Example",id:"domain-randomization-example",level:3},{value:"Isaac ROS Apriltag Detection",id:"isaac-ros-apriltag-detection",level:2},{value:"Apriltag Detection Node",id:"apriltag-detection-node",level:3},{value:"Isaac Lab Example: Robot Learning Environment",id:"isaac-lab-example-robot-learning-environment",level:2},{value:"Simple Navigation Task",id:"simple-navigation-task",level:3},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Practical Exercises",id:"practical-exercises",level:2},{value:"Common Failure Modes",id:"common-failure-modes",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"code-examples-isaac-ros-pipelines-and-ai-perception",children:"Code Examples: Isaac ROS Pipelines and AI Perception"})}),"\n",(0,t.jsx)(n.h2,{id:"isaac-ros-vslam-pipeline",children:"Isaac ROS VSLAM Pipeline"}),"\n",(0,t.jsx)(n.h3,{id:"basic-vslam-node-implementation",children:"Basic VSLAM Node Implementation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, Imu, CameraInfo\nfrom geometry_msgs.msg import PoseStamped\nfrom nav_msgs.msg import Odometry\nimport cv2\nfrom cv_bridge import CvBridge\nimport numpy as np\n\n\nclass IsaacVSLAMNode(Node):\n\n    def __init__(self):\n        super().__init__(\'isaac_vslam_node\')\n\n        # Publishers\n        self.pose_pub = self.create_publisher(PoseStamped, \'visual_slam/pose\', 10)\n        self.odom_pub = self.create_publisher(Odometry, \'visual_slam/odometry\', 10)\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/camera/image_raw\',\n            self.image_callback,\n            10)\n\n        self.imu_sub = self.create_subscription(\n            Imu,\n            \'/imu/data\',\n            self.imu_callback,\n            10)\n\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo,\n            \'/camera/camera_info\',\n            self.camera_info_callback,\n            10)\n\n        self.bridge = CvBridge()\n        self.camera_matrix = None\n        self.distortion_coeffs = None\n\n        # Feature tracking variables\n        self.prev_image = None\n        self.prev_keypoints = None\n        self.camera_pose = np.eye(4)\n\n        self.get_logger().info(\'Isaac VSLAM Node initialized\')\n\n    def camera_info_callback(self, msg):\n        """Process camera calibration information"""\n        if self.camera_matrix is None:\n            self.camera_matrix = np.array(msg.k).reshape(3, 3)\n            self.distortion_coeffs = np.array(msg.d)\n            self.get_logger().info(f\'Camera matrix set: {self.camera_matrix}\')\n\n    def image_callback(self, msg):\n        """Process incoming camera images for VSLAM"""\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\n\n            # Convert to grayscale for feature detection\n            gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\n\n            # Detect and track features\n            if self.prev_image is None:\n                # Initialize first frame\n                self.prev_image = gray\n                self.prev_keypoints = cv2.goodFeaturesToTrack(\n                    gray, maxCorners=100, qualityLevel=0.01, minDistance=10)\n            else:\n                # Track features from previous frame\n                if self.prev_keypoints is not None:\n                    # Calculate optical flow\n                    next_keypoints, status, error = cv2.calcOpticalFlowPyrLK(\n                        self.prev_image, gray, self.prev_keypoints, None)\n\n                    # Filter valid points\n                    valid = status.ravel() == 1\n                    prev_valid = self.prev_keypoints[valid]\n                    next_valid = next_keypoints[valid]\n\n                    if len(prev_valid) >= 8:  # Need at least 8 points for pose estimation\n                        # Estimate essential matrix\n                        E, mask = cv2.findEssentialMat(\n                            next_valid, prev_valid, self.camera_matrix,\n                            method=cv2.RANSAC, prob=0.999, threshold=1.0)\n\n                        if E is not None:\n                            # Recover pose\n                            _, R, t, mask_pose = cv2.recoverPose(\n                                E, next_valid, prev_valid, self.camera_matrix)\n\n                            # Update camera pose\n                            delta_transform = np.eye(4)\n                            delta_transform[:3, :3] = R\n                            delta_transform[:3, 3] = t.ravel()\n\n                            self.camera_pose = self.camera_pose @ np.linalg.inv(delta_transform)\n\n                    # Update for next iteration\n                    self.prev_image = gray\n                    self.prev_keypoints = next_valid.reshape(-1, 1, 2)\n\n                # Publish current pose\n                self.publish_pose()\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing image: {e}\')\n\n    def imu_callback(self, msg):\n        """Process IMU data to improve VSLAM"""\n        # Use IMU data to improve pose estimation\n        # This is a simplified example - real implementation would use sensor fusion\n        pass\n\n    def publish_pose(self):\n        """Publish the estimated camera pose"""\n        pose_msg = PoseStamped()\n        pose_msg.header.stamp = self.get_clock().now().to_msg()\n        pose_msg.header.frame_id = \'map\'\n\n        # Extract position and orientation from transformation matrix\n        position = self.camera_pose[:3, 3]\n        pose_msg.pose.position.x = float(position[0])\n        pose_msg.pose.position.y = float(position[1])\n        pose_msg.pose.position.z = float(position[2])\n\n        # Convert rotation matrix to quaternion\n        rotation_matrix = self.camera_pose[:3, :3]\n        qw, qx, qy, qz = self.rotation_matrix_to_quaternion(rotation_matrix)\n        pose_msg.pose.orientation.w = qw\n        pose_msg.pose.orientation.x = qx\n        pose_msg.pose.orientation.y = qy\n        pose_msg.pose.orientation.z = qz\n\n        self.pose_pub.publish(pose_msg)\n\n        # Also publish as odometry\n        odom_msg = Odometry()\n        odom_msg.header = pose_msg.header\n        odom_msg.child_frame_id = \'base_link\'\n        odom_msg.pose.pose = pose_msg.pose\n        self.odom_pub.publish(odom_msg)\n\n    def rotation_matrix_to_quaternion(self, R):\n        """Convert rotation matrix to quaternion"""\n        trace = np.trace(R)\n        if trace > 0:\n            s = np.sqrt(trace + 1.0) * 2  # s = 4 * qw\n            qw = 0.25 * s\n            qx = (R[2, 1] - R[1, 2]) / s\n            qy = (R[0, 2] - R[2, 0]) / s\n            qz = (R[1, 0] - R[0, 1]) / s\n        else:\n            if R[0, 0] > R[1, 1] and R[0, 0] > R[2, 2]:\n                s = np.sqrt(1.0 + R[0, 0] - R[1, 1] - R[2, 2]) * 2\n                qw = (R[2, 1] - R[1, 2]) / s\n                qx = 0.25 * s\n                qy = (R[0, 1] + R[1, 0]) / s\n                qz = (R[0, 2] + R[2, 0]) / s\n            elif R[1, 1] > R[2, 2]:\n                s = np.sqrt(1.0 + R[1, 1] - R[0, 0] - R[2, 2]) * 2\n                qw = (R[0, 2] - R[2, 0]) / s\n                qx = (R[0, 1] + R[1, 0]) / s\n                qy = 0.25 * s\n                qz = (R[1, 2] + R[2, 1]) / s\n            else:\n                s = np.sqrt(1.0 + R[2, 2] - R[0, 0] - R[1, 1]) * 2\n                qw = (R[1, 0] - R[0, 1]) / s\n                qx = (R[0, 2] + R[2, 0]) / s\n                qy = (R[1, 2] + R[2, 1]) / s\n                qz = 0.25 * s\n\n        # Normalize quaternion\n        norm = np.sqrt(qw*qw + qx*qx + qy*qy + qz*qz)\n        return qw/norm, qx/norm, qy/norm, qz/norm\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    vslam_node = IsaacVSLAMNode()\n\n    try:\n        rclpy.spin(vslam_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        vslam_node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"isaac-ros-stereo-dnn-pipeline",children:"Isaac ROS Stereo DNN Pipeline"}),"\n",(0,t.jsx)(n.h3,{id:"stereo-dnn-node-implementation",children:"Stereo DNN Node Implementation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom stereo_msgs.msg import DisparityImage\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport cv2\n\n\nclass IsaacStereoDNNNode(Node):\n\n    def __init__(self):\n        super().__init__('isaac_stereo_dnn_node')\n\n        # Publishers\n        self.disparity_pub = self.create_publisher(DisparityImage, '/disparity', 10)\n        self.depth_pub = self.create_publisher(Image, '/depth', 10)\n\n        # Subscribers for stereo pair\n        self.left_sub = self.create_subscription(\n            Image,\n            '/stereo_camera/left/image_raw',\n            self.left_image_callback,\n            10)\n\n        self.right_sub = self.create_subscription(\n            Image,\n            '/stereo_camera/right/image_raw',\n            self.right_image_callback,\n            10)\n\n        self.bridge = CvBridge()\n        self.left_image = None\n        self.right_image = None\n        self.camera_baseline = 0.1  # Baseline in meters\n        self.focal_length = 320.0   # Focal length in pixels (example value)\n\n        self.get_logger().info('Isaac Stereo DNN Node initialized')\n\n    def left_image_callback(self, msg):\n        \"\"\"Process left camera image\"\"\"\n        try:\n            self.left_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n            self.process_stereo()\n        except Exception as e:\n            self.get_logger().error(f'Error processing left image: {e}')\n\n    def right_image_callback(self, msg):\n        \"\"\"Process right camera image\"\"\"\n        try:\n            self.right_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n            self.process_stereo()\n        except Exception as e:\n            self.get_logger().error(f'Error processing right image: {e}')\n\n    def process_stereo(self):\n        \"\"\"Process stereo pair to generate disparity and depth\"\"\"\n        if self.left_image is None or self.right_image is None:\n            return\n\n        # Convert to grayscale\n        left_gray = cv2.cvtColor(self.left_image, cv2.COLOR_BGR2GRAY)\n        right_gray = cv2.cvtColor(self.right_image, cv2.COLOR_BGR2GRAY)\n\n        # Create stereo matcher (using SGBM as example)\n        stereo = cv2.StereoSGBM_create(\n            minDisparity=0,\n            numDisparities=128,\n            blockSize=5,\n            P1=8 * 3 * 5**2,\n            P2=32 * 3 * 5**2,\n            disp12MaxDiff=1,\n            uniquenessRatio=15,\n            speckleWindowSize=0,\n            speckleRange=2,\n            preFilterCap=63,\n            mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY\n        )\n\n        # Compute disparity\n        disparity = stereo.compute(left_gray, right_gray).astype(np.float32)\n\n        # Convert disparity to depth\n        # Depth = (baseline * focal_length) / disparity\n        depth = np.zeros_like(disparity)\n        valid_disparity = disparity > 0\n        depth[valid_disparity] = (self.camera_baseline * self.focal_length) / disparity[valid_disparity]\n\n        # Publish disparity image\n        disparity_msg = DisparityImage()\n        disparity_msg.header.stamp = self.get_clock().now().to_msg()\n        disparity_msg.header.frame_id = 'stereo_camera'\n        disparity_msg.image = self.bridge.cv2_to_imgmsg(disparity, encoding='32FC1')\n        disparity_msg.f = self.focal_length\n        disparity_msg.T = self.camera_baseline\n        disparity_msg.min_disparity = 0.0\n        disparity_msg.max_disparity = 128.0\n        disparity_msg.delta_d = 0.1666666\n        self.disparity_pub.publish(disparity_msg)\n\n        # Publish depth image\n        depth_msg = self.bridge.cv2_to_imgmsg(depth, encoding='32FC1')\n        depth_msg.header.stamp = disparity_msg.header.stamp\n        depth_msg.header.frame_id = disparity_msg.header.frame_id\n        self.depth_pub.publish(depth_msg)\n\n        self.get_logger().info('Stereo processing completed')\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    stereo_node = IsaacStereoDNNNode()\n\n    try:\n        rclpy.spin(stereo_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        stereo_node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"synthetic-data-generation-script",children:"Synthetic Data Generation Script"}),"\n",(0,t.jsx)(n.h3,{id:"domain-randomization-example",children:"Domain Randomization Example"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import omni\nfrom pxr import UsdGeom, Gf, Sdf\nimport numpy as np\nimport random\nimport carb\n\n\nclass SyntheticDataGenerator:\n    def __init__(self):\n        self.stage = omni.usd.get_context().get_stage()\n        self.randomization_params = {\n            'lighting': {\n                'intensity_range': (0.5, 2.0),\n                'color_temperature_range': (3000, 8000)\n            },\n            'materials': {\n                'roughness_range': (0.1, 0.9),\n                'metallic_range': (0.0, 0.2),\n                'specular_range': (0.5, 1.0)\n            },\n            'objects': {\n                'position_jitter': 0.1,\n                'rotation_jitter': 0.1\n            }\n        }\n\n    def randomize_lighting(self):\n        \"\"\"Randomize lighting conditions in the scene\"\"\"\n        # Find all lights in the scene\n        light_prims = []\n        for prim in self.stage.Traverse():\n            if prim.GetTypeName() in ['DistantLight', 'SphereLight', 'RectLight']:\n                light_prims.append(prim)\n\n        for light_prim in light_prims:\n            # Randomize light intensity\n            intensity = random.uniform(\n                self.randomization_params['lighting']['intensity_range'][0],\n                self.randomization_params['lighting']['intensity_range'][1]\n            )\n            intensity_attr = light_prim.GetAttribute('inputs:intensity')\n            if not intensity_attr:\n                intensity_attr = light_prim.CreateAttribute('inputs:intensity', Sdf.ValueTypeNames.Float)\n            intensity_attr.Set(intensity)\n\n            # Randomize color temperature\n            color_temp = random.uniform(\n                self.randomization_params['lighting']['color_temperature_range'][0],\n                self.randomization_params['lighting']['color_temperature_range'][1]\n            )\n            # Convert color temperature to RGB (simplified)\n            rgb = self.color_temperature_to_rgb(color_temp)\n            color_attr = light_prim.GetAttribute('inputs:color')\n            if not color_attr:\n                color_attr = light_prim.CreateAttribute('inputs:color', Sdf.ValueTypeNames.Color3f)\n            color_attr.Set(Gf.Vec3f(rgb[0], rgb[1], rgb[2]))\n\n    def randomize_materials(self):\n        \"\"\"Randomize material properties\"\"\"\n        # Find all materials in the scene\n        material_prims = []\n        for prim in self.stage.Traverse():\n            if prim.GetTypeName() == 'Material':\n                material_prims.append(prim)\n\n        for material_prim in material_prims:\n            # Randomize surface shader properties\n            surface_shader = material_prim.GetChildren()\n            for shader in surface_shader:\n                if shader.GetTypeName() == 'Shader':\n                    # Randomize roughness\n                    roughness = random.uniform(\n                        self.randomization_params['materials']['roughness_range'][0],\n                        self.randomization_params['materials']['roughness_range'][1]\n                    )\n                    roughness_attr = shader.GetAttribute('inputs:roughness')\n                    if roughness_attr:\n                        roughness_attr.Set(roughness)\n\n                    # Randomize metallic\n                    metallic = random.uniform(\n                        self.randomization_params['materials']['metallic_range'][0],\n                        self.randomization_params['materials']['metallic_range'][1]\n                    )\n                    metallic_attr = shader.GetAttribute('inputs:metallic')\n                    if metallic_attr:\n                        metallic_attr.Set(metallic)\n\n    def randomize_objects(self):\n        \"\"\"Randomize object positions and properties\"\"\"\n        # Find all geometry objects\n        geometry_prims = []\n        for prim in self.stage.Traverse():\n            if prim.GetTypeName() in ['Mesh', 'Cube', 'Sphere', 'Cylinder']:\n                geometry_prims.append(prim)\n\n        for geom_prim in geometry_prims:\n            # Get current transform\n            xformable = UsdGeom.Xformable(geom_prim)\n            transform = xformable.ComputeLocalToWorldTransform(0)\n\n            # Add random position jitter\n            pos_jitter = self.randomization_params['objects']['position_jitter']\n            jitter = Gf.Vec3d(\n                random.uniform(-pos_jitter, pos_jitter),\n                random.uniform(-pos_jitter, pos_jitter),\n                random.uniform(-pos_jitter, pos_jitter)\n            )\n\n            # Apply jitter to position\n            new_transform = transform\n            new_transform.SetTranslate(transform.ExtractTranslation() + jitter)\n\n            # Apply new transform\n            xformable.SetTransform(new_transform)\n\n    def color_temperature_to_rgb(self, temperature):\n        \"\"\"Convert color temperature in Kelvin to RGB values\"\"\"\n        temperature = max(1000, min(40000, temperature)) / 100.0\n\n        if temperature <= 66:\n            red = 255\n        else:\n            red = temperature - 60\n            red = 329.698727446 * (red ** -0.1332047592)\n            red = max(0, min(255, red))\n\n        if temperature <= 66:\n            green = temperature\n            green = 99.4708025861 * np.log(green) - 161.1195681661\n        else:\n            green = temperature - 60\n            green = 288.1221695283 * (green ** -0.0755148492)\n        green = max(0, min(255, green))\n\n        if temperature >= 66:\n            blue = 255\n        elif temperature <= 19:\n            blue = 0\n        else:\n            blue = temperature - 10\n            blue = 138.5177312231 * np.log(blue) - 305.0447927307\n            blue = max(0, min(255, blue))\n\n        return [red/255.0, green/255.0, blue/255.0]\n\n    def generate_batch(self, num_samples):\n        \"\"\"Generate a batch of synthetic data\"\"\"\n        for i in range(num_samples):\n            # Apply randomizations\n            self.randomize_lighting()\n            self.randomize_materials()\n            self.randomize_objects()\n\n            # Capture data (this would be done through Isaac Sim's capture tools)\n            print(f\"Generated sample {i+1}/{num_samples}\")\n\n\ndef run_synthetic_data_generation():\n    \"\"\"Run the synthetic data generation process\"\"\"\n    generator = SyntheticDataGenerator()\n    generator.generate_batch(100)  # Generate 100 samples\n    print(\"Synthetic data generation completed\")\n\n\nif __name__ == \"__main__\":\n    run_synthetic_data_generation()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"isaac-ros-apriltag-detection",children:"Isaac ROS Apriltag Detection"}),"\n",(0,t.jsx)(n.h3,{id:"apriltag-detection-node",children:"Apriltag Detection Node"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import PoseArray, Pose\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nimport pupil_apriltags as apriltag  # Using the Python Apriltag library\n\n\nclass IsaacApriltagNode(Node):\n\n    def __init__(self):\n        super().__init__('isaac_apriltag_node')\n\n        # Publishers\n        self.detections_pub = self.create_publisher(PoseArray, '/apriltag_detections', 10)\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.image_callback,\n            10)\n\n        self.bridge = CvBridge()\n\n        # Apriltag detector\n        self.detector = apriltag.Detector(families='tag36h11')\n\n        # Camera parameters (these should match your camera calibration)\n        self.camera_matrix = np.array([\n            [320.0, 0.0, 320.0],   # fx, 0, cx\n            [0.0, 320.0, 240.0],   # 0, fy, cy\n            [0.0, 0.0, 1.0]        # 0, 0, 1\n        ])\n\n        # Tag size in meters (for pose estimation)\n        self.tag_size = 0.16  # 16cm tag\n\n        self.get_logger().info('Isaac Apriltag Node initialized')\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming images for Apriltag detection\"\"\"\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n            # Convert to grayscale for Apriltag detection\n            gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\n\n            # Detect Apriltags\n            tags = self.detector.detect(\n                gray,\n                estimate_tag_pose=True,\n                camera_params=[self.camera_matrix[0, 0], self.camera_matrix[1, 1],\n                              self.camera_matrix[0, 2], self.camera_matrix[1, 2]],\n                tag_size=self.tag_size\n            )\n\n            # Create PoseArray message\n            pose_array = PoseArray()\n            pose_array.header.stamp = msg.header.stamp\n            pose_array.header.frame_id = msg.header.frame_id\n\n            for tag in tags:\n                # Create pose for detected tag\n                pose = Pose()\n\n                # Position from tag pose (in camera frame)\n                pose.position.x = float(tag.pose_t[0])\n                pose.position.y = float(tag.pose_t[1])\n                pose.position.z = float(tag.pose_t[2])\n\n                # Orientation from tag rotation matrix\n                rotation_matrix = tag.pose_R\n                qw, qx, qy, qz = self.rotation_matrix_to_quaternion(rotation_matrix)\n                pose.orientation.w = qw\n                pose.orientation.x = qx\n                pose.orientation.y = qy\n                pose.orientation.z = qz\n\n                pose_array.poses.append(pose)\n\n                # Optional: Draw detected tags on image for visualization\n                for idx in range(len(tag.corners)):\n                    pt1 = tuple(tag.corners[idx][0].astype(int))\n                    pt2 = tuple(tag.corners[(idx + 1) % len(tag.corners)][0].astype(int))\n                    cv2.line(cv_image, pt1, pt2, (0, 255, 0), 2)\n\n                # Put tag ID on image\n                cv2.putText(cv_image, str(tag.tag_id),\n                           tuple(tag.center.astype(int)),\n                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n            # Publish detections\n            self.detections_pub.publish(pose_array)\n\n            if len(tags) > 0:\n                self.get_logger().info(f'Detected {len(tags)} Apriltags')\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n\n    def rotation_matrix_to_quaternion(self, R):\n        \"\"\"Convert rotation matrix to quaternion\"\"\"\n        trace = np.trace(R)\n        if trace > 0:\n            s = np.sqrt(trace + 1.0) * 2  # s = 4 * qw\n            qw = 0.25 * s\n            qx = (R[2, 1] - R[1, 2]) / s\n            qy = (R[0, 2] - R[2, 0]) / s\n            qz = (R[1, 0] - R[0, 1]) / s\n        else:\n            if R[0, 0] > R[1, 1] and R[0, 0] > R[2, 2]:\n                s = np.sqrt(1.0 + R[0, 0] - R[1, 1] - R[2, 2]) * 2\n                qw = (R[2, 1] - R[1, 2]) / s\n                qx = 0.25 * s\n                qy = (R[0, 1] + R[1, 0]) / s\n                qz = (R[0, 2] + R[2, 0]) / s\n            elif R[1, 1] > R[2, 2]:\n                s = np.sqrt(1.0 + R[1, 1] - R[0, 0] - R[2, 2]) * 2\n                qw = (R[0, 2] - R[2, 0]) / s\n                qx = (R[0, 1] + R[1, 0]) / s\n                qy = 0.25 * s\n                qz = (R[1, 2] + R[2, 1]) / s\n            else:\n                s = np.sqrt(1.0 + R[2, 2] - R[0, 0] - R[1, 1]) * 2\n                qw = (R[1, 0] - R[0, 1]) / s\n                qx = (R[0, 2] + R[2, 0]) / s\n                qy = (R[1, 2] + R[2, 1]) / s\n                qz = 0.25 * s\n\n        # Normalize quaternion\n        norm = np.sqrt(qw*qw + qx*qx + qy*qy + qz*qz)\n        return qw/norm, qx/norm, qy/norm, qz/norm\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    apriltag_node = IsaacApriltagNode()\n\n    try:\n        rclpy.spin(apriltag_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        apriltag_node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"isaac-lab-example-robot-learning-environment",children:"Isaac Lab Example: Robot Learning Environment"}),"\n",(0,t.jsx)(n.h3,{id:"simple-navigation-task",children:"Simple Navigation Task"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import torch\nimport numpy as np\nfrom omni.isaac.orbit.assets import RigidObject, Articulation\nfrom omni.isaac.orbit.controllers import DifferentialController\nfrom omni.isaac.orbit.envs import RLTask\nfrom omni.isaac.orbit.utils import configclass\nfrom omni.isaac.orbit.tasks.locomotion.velocity import LocomotionVelocityRoughEnvCfg\nfrom omni.isaac.orbit.assets import AssetBaseCfg\n\n\n@configclass\nclass HumanoidNavigationEnvCfg(LocomotionVelocityRoughEnvCfg):\n    """Configuration for the humanoid navigation environment."""\n\n    def __post_init__(self):\n        # Call parent configuration\n        super().__post_init__()\n\n        # Override scene parameters\n        self.scene.num_envs = 512\n        self.scene.env_spacing = 4.0\n\n        # Add humanoid robot\n        self.scene.robot = ArticulationCfg(\n            prim_path="{ENV_REGEX_NS}/Robot",\n            spawn=sim_utils.UsdFileCfg(\n                usd_path="/path/to/humanoid/robot.usd",\n                activate_contact_sensors=True,\n                rigid_props=sim_utils.RigidBodyPropertiesCfg(\n                    disable_gravity=False,\n                    max_depenetration_velocity=1.0,\n                ),\n                articulation_props=sim_utils.ArticulationRootPropertiesCfg(\n                    enabled_self_collisions=True, solver_position_iteration_count=4,\n                    solver_velocity_iteration_count=0,\n                ),\n            ),\n            init_state=ArticulationCfg.InitialStateCfg(\n                pos=(0.0, 0.0, 1.0),\n                joint_pos={\n                    ".*L_HIP_JOINT": 0.0,\n                    ".*R_HIP_JOINT": 0.0,\n                    ".*L_KNEE_JOINT": 0.0,\n                    ".*R_KNEE_JOINT": 0.0,\n                    ".*L_ANKLE_JOINT": 0.0,\n                    ".*R_ANKLE_JOINT": 0.0,\n                },\n            ),\n            actuator_cfg=BiDifferentialAccessCfg(\n                joint_names_expr=[".*_HIP_JOINT", ".*_KNEE_JOINT", ".*_ANKLE_JOINT"],\n                effort_limit=80.0,\n                velocity_limit=100.0,\n                stiffness=200.0,\n                damping=10.0,\n            ),\n        )\n\n\nclass HumanoidNavigationTask(RLTask):\n    """Humanoid navigation task for reinforcement learning."""\n\n    def __init__(self, cfg: HumanoidNavigationEnvCfg, env: ManagerBasedRLEnv):\n        super().__init__(cfg=cfg, env=env)\n\n        # Initialize action and observation spaces\n        self.action_space = torch.nn.Linear(self.num_actions, 12)  # Example: 12 joint commands\n        self.observation_space = torch.nn.Linear(self.num_obs, 48)  # Example: 48 observation dimensions\n\n    def set_episode_end(self, env_ids):\n        """Reset environments that reached the end of an episode."""\n        super().set_episode_end(env_ids)\n\n        # Additional reset logic specific to humanoid navigation\n        self.reset_robot_positions(env_ids)\n\n    def get_observations(self) -> dict:\n        """Get observations for the current step."""\n        # Example observation: joint positions, velocities, IMU data, target position\n        obs = torch.concat([\n            self.robot.data.joint_pos_history,      # Joint positions\n            self.robot.data.joint_vel_history,      # Joint velocities\n            self.robot.data.imu_ang_vel_history,    # IMU angular velocity\n            self.robot.data.imu_lin_acc_history,    # IMU linear acceleration\n            self.target_pos - self.robot.data.body_pos_w,  # Relative target position\n        ], dim=-1)\n\n        return {"policy": obs}\n\n    def get_rewards(self) -> torch.Tensor:\n        """Get rewards for the current step."""\n        # Example reward function\n        lin_vel_error = torch.sum(torch.square(self.target_lin_vel - self.robot.data.body_lin_vel_w[:, :2]), dim=1)\n        ang_vel_error = torch.square(self.target_ang_vel - self.robot.data.body_ang_vel_w[:, 2])\n\n        rew = -lin_vel_error - 0.1 * ang_vel_error\n\n        return rew\n\n\ndef create_humanoid_navigation_env():\n    """Create and configure the humanoid navigation environment."""\n    # Create environment configuration\n    env_cfg = HumanoidNavigationEnvCfg()\n\n    # Create the environment\n    env = ManagerBasedRLEnv(\n        cfg=env_cfg,\n        render_mode="rgb_array",\n        sim_params={"substeps": 2}\n    )\n\n    # Create the task\n    task = HumanoidNavigationTask(cfg=env_cfg, env=env)\n\n    return env, task\n\n\ndef train_humanoid_navigation():\n    """Example training loop for humanoid navigation."""\n    env, task = create_humanoid_navigation_env()\n\n    # Initialize RL agent (example with PPO)\n    agent = PPO(env, "MlpPolicy", verbose=1)\n\n    # Train the agent\n    agent.learn(total_timesteps=1000000)\n\n    # Save the trained model\n    agent.save("humanoid_navigation_ppo")\n\n    # Close the environment\n    env.close()\n\n    print("Training completed and model saved!")\n\n\nif __name__ == "__main__":\n    train_humanoid_navigation()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Isaac ROS VSLAM"}),": Hardware-accelerated visual SLAM pipeline"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Stereo DNN"}),": Deep neural network processing for stereo vision"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Apriltag Detection"}),": GPU-accelerated fiducial marker detection"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Synthetic Data Generation"}),": Creating labeled training data in simulation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Domain Randomization"}),": Technique to improve real-world transfer of AI models"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"NITROS"}),": NVIDIA's Inter-Process Communication framework for ROS"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Reinforcement Learning"}),": Training policies through environmental interaction"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Hardware Acceleration"}),": GPU utilization for real-time AI processing"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"practical-exercises",children:"Practical Exercises"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Implement a complete Isaac ROS VSLAM pipeline with your robot"}),"\n",(0,t.jsx)(n.li,{children:"Create a synthetic data generation pipeline for object detection"}),"\n",(0,t.jsx)(n.li,{children:"Train a perception model using Isaac Sim and synthetic data"}),"\n",(0,t.jsx)(n.li,{children:"Implement a reinforcement learning task for humanoid locomotion"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"common-failure-modes",children:"Common Failure Modes"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"GPU Resource Exhaustion"}),": Complex pipelines consuming excessive GPU memory"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Pose Estimation Errors"}),": Incorrect camera calibration causing poor tracking"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Feature Tracking Failures"}),": Insufficient texture in environment for VSLAM"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Domain Gap"}),": Poor transfer from synthetic to real data"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Real-time Performance"}),": Pipelines not meeting timing requirements"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Calibration Issues"}),": Incorrect camera or sensor parameters causing errors"]}),"\n"]})]})}function c(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}},8453(e,n,i){i.d(n,{R:()=>s,x:()=>o});var a=i(6540);const t={},r=a.createContext(t);function s(e){const n=a.useContext(r);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),a.createElement(r.Provider,{value:n},e.children)}}}]);