"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[968],{1922(e,n,a){a.r(n),a.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>p,frontMatter:()=>i,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"module-4-vla/hands-on","title":"Hands-On: Vision-Language-Action Implementation","description":"Exercise 1: Setting Up Voice Command Ingestion System","source":"@site/docs/module-4-vla/hands-on.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/hands-on","permalink":"/docs/module-4-vla/hands-on","draft":false,"unlisted":false,"editUrl":"https://github.com/Farhan899/Ai-Robotics-Book/edit/main/docs/module-4-vla/hands-on.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Vision-Language-Action (VLA) Architecture","permalink":"/docs/module-4-vla/architecture"},"next":{"title":"Code Examples: LLM Integration and ROS 2 Action Mapping","permalink":"/docs/module-4-vla/code-examples"}}');var s=a(4848),o=a(8453);const i={},r="Hands-On: Vision-Language-Action Implementation",c={},l=[{value:"Exercise 1: Setting Up Voice Command Ingestion System",id:"exercise-1-setting-up-voice-command-ingestion-system",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Steps",id:"steps",level:3},{value:"Exercise 2: Implementing Natural Language Command Parsing",id:"exercise-2-implementing-natural-language-command-parsing",level:2},{value:"Steps",id:"steps-1",level:3},{value:"Exercise 3: Creating a Task Decomposition System",id:"exercise-3-creating-a-task-decomposition-system",level:2},{value:"Steps",id:"steps-2",level:3},{value:"Exercise 4: Building Language-to-ROS Action Mapping System",id:"exercise-4-building-language-to-ros-action-mapping-system",level:2},{value:"Steps",id:"steps-3",level:3},{value:"Exercise 5: Complete VLA Integration",id:"exercise-5-complete-vla-integration",level:2},{value:"Steps",id:"steps-4",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Practical Exercises",id:"practical-exercises",level:2},{value:"Common Failure Modes",id:"common-failure-modes",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"hands-on-vision-language-action-implementation",children:"Hands-On: Vision-Language-Action Implementation"})}),"\n",(0,s.jsx)(n.h2,{id:"exercise-1-setting-up-voice-command-ingestion-system",children:"Exercise 1: Setting Up Voice Command Ingestion System"}),"\n",(0,s.jsx)(n.p,{children:"In this exercise, you'll set up a complete voice command ingestion system that converts speech to text and prepares it for processing."}),"\n",(0,s.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"ROS 2 Humble installed"}),"\n",(0,s.jsx)(n.li,{children:"Microphone connected to your system"}),"\n",(0,s.jsx)(n.li,{children:"Python 3.8+ with required packages"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"steps",children:"Steps"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Install required packages for speech recognition:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip3 install speechrecognition pyaudio vosk transformers torch\n# For OpenAI Whisper (optional):\npip3 install openai-whisper\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Create a ROS 2 package for VLA components:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"mkdir -p ~/vla_ws/src\ncd ~/vla_ws/src\nros2 pkg create --build-type ament_python vla_voice_ingestion\ncd vla_voice_ingestion\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Create the voice ingestion node ",(0,s.jsx)(n.code,{children:"voice_ingestion_node.py"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nimport speech_recognition as sr\nimport threading\nimport queue\nfrom std_msgs.msg import String\nfrom audio_common_msgs.msg import AudioData\n\n\nclass VoiceIngestionNode(Node):\n\n    def __init__(self):\n        super().__init__('voice_ingestion_node')\n\n        # Publishers\n        self.command_pub = self.create_publisher(String, 'voice_commands', 10)\n\n        # Parameters\n        self.silence_threshold = self.declare_parameter('silence_threshold', 500).value\n        self.phrase_time_limit = self.declare_parameter('phrase_time_limit', 5.0).value\n\n        # Speech recognition setup\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n\n        # Adjust for ambient noise\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source)\n\n        # Start voice processing thread\n        self.audio_queue = queue.Queue()\n        self.listening_thread = threading.Thread(target=self.listen_continuously)\n        self.listening_thread.daemon = True\n        self.listening_thread.start()\n\n        self.get_logger().info('Voice ingestion node initialized')\n\n    def listen_continuously(self):\n        \"\"\"Continuously listen for voice commands\"\"\"\n        with self.microphone as source:\n            self.get_logger().info(\"Listening for voice commands...\")\n            while rclpy.ok():\n                try:\n                    # Listen for audio with timeout\n                    audio = self.recognizer.listen(\n                        source,\n                        timeout=1.0,\n                        phrase_time_limit=self.phrase_time_limit\n                    )\n\n                    # Add audio to queue for processing\n                    self.audio_queue.put(audio)\n\n                except sr.WaitTimeoutError:\n                    # Continue listening\n                    continue\n                except Exception as e:\n                    self.get_logger().error(f'Error in listening: {e}')\n                    continue\n\n    def process_audio(self):\n        \"\"\"Process audio from queue\"\"\"\n        try:\n            while not self.audio_queue.empty():\n                audio = self.audio_queue.get_nowait()\n\n                # Try to recognize speech using Google (online)\n                try:\n                    text = self.recognizer.recognize_google(audio)\n                    self.get_logger().info(f'Recognized: {text}')\n\n                    # Publish recognized command\n                    cmd_msg = String()\n                    cmd_msg.data = text\n                    self.command_pub.publish(cmd_msg)\n\n                except sr.UnknownValueError:\n                    self.get_logger().info('Could not understand audio')\n                except sr.RequestError as e:\n                    self.get_logger().error(f'Could not request results: {e}')\n\n        except queue.Empty:\n            pass  # No audio in queue\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VoiceIngestionNode()\n\n    try:\n        while rclpy.ok():\n            node.process_audio()\n            rclpy.spin_once(node, timeout_sec=0.1)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Make the node executable and test it:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"chmod +x voice_ingestion_node.py\ncd ~/vla_ws\ncolcon build --packages-select vla_voice_ingestion\nsource install/setup.bash\nros2 run vla_voice_ingestion voice_ingestion_node.py\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"In another terminal, listen to recognized commands:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 topic echo /voice_commands std_msgs/msg/String\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"exercise-2-implementing-natural-language-command-parsing",children:"Exercise 2: Implementing Natural Language Command Parsing"}),"\n",(0,s.jsx)(n.p,{children:"Now you'll implement a natural language command parser that converts text commands into structured representations."}),"\n",(0,s.jsx)(n.h3,{id:"steps-1",children:"Steps"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Create a command parsing node ",(0,s.jsx)(n.code,{children:"command_parser_node.py"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Pose\nfrom vla_interfaces.msg import ParsedCommand  # You'll need to create this message type\nimport re\nimport json\n\n\nclass CommandParserNode(Node):\n\n    def __init__(self):\n        super().__init__('command_parser_node')\n\n        # Publishers and Subscribers\n        self.command_sub = self.create_subscription(\n            String, 'voice_commands', self.command_callback, 10)\n        self.parsed_pub = self.create_publisher(\n            ParsedCommand, 'parsed_commands', 10)\n\n        # Define command patterns\n        self.command_patterns = {\n            'move': [\n                r'move to (.+)',\n                r'go to (.+)',\n                r'navigate to (.+)',\n                r'go (.+)',\n                r'move (.+)'\n            ],\n            'grasp': [\n                r'pick up (.+)',\n                r'grasp (.+)',\n                r'pick (.+)',\n                r'get (.+)',\n                r'grab (.+)'\n            ],\n            'place': [\n                r'place (.+) at (.+)',\n                r'put (.+) at (.+)',\n                r'place (.+) on (.+)',\n                r'put (.+) on (.+)'\n            ],\n            'find': [\n                r'find (.+)',\n                r'locate (.+)',\n                r'where is (.+)',\n                r'search for (.+)'\n            ]\n        }\n\n        # Define location keywords\n        self.location_keywords = {\n            'kitchen': [0.0, 0.0, 0.0],  # Replace with actual coordinates\n            'living room': [2.0, 0.0, 0.0],\n            'bedroom': [0.0, 2.0, 0.0],\n            'table': [1.0, 1.0, 0.0],\n            'couch': [2.0, 1.0, 0.0]\n        }\n\n        # Define object keywords\n        self.object_keywords = [\n            'cup', 'bottle', 'book', 'phone', 'keys', 'apple',\n            'banana', 'box', 'toy', 'remote'\n        ]\n\n        self.get_logger().info('Command parser node initialized')\n\n    def command_callback(self, msg):\n        \"\"\"Parse incoming voice commands\"\"\"\n        command_text = msg.data.lower().strip()\n        self.get_logger().info(f'Parsing command: {command_text}')\n\n        # Parse the command\n        parsed_cmd = self.parse_command(command_text)\n\n        if parsed_cmd:\n            # Publish parsed command\n            self.parsed_pub.publish(parsed_cmd)\n            self.get_logger().info(f'Parsed command: {parsed_cmd.action} {parsed_cmd.parameters}')\n        else:\n            self.get_logger().warn(f'Could not parse command: {command_text}')\n\n    def parse_command(self, command):\n        \"\"\"Parse natural language command into structured format\"\"\"\n        # Try each command type\n        for action_type, patterns in self.command_patterns.items():\n            for pattern in patterns:\n                match = re.search(pattern, command)\n                if match:\n                    params = list(match.groups())\n\n                    # Create parsed command message\n                    parsed_cmd = ParsedCommand()\n                    parsed_cmd.action = action_type\n                    parsed_cmd.raw_command = command\n\n                    # Process parameters based on action type\n                    if action_type == 'move':\n                        parsed_cmd.target_location = self.extract_location(params[0])\n                    elif action_type == 'grasp':\n                        parsed_cmd.target_object = self.extract_object(params[0])\n                    elif action_type == 'place':\n                        parsed_cmd.target_object = self.extract_object(params[0])\n                        parsed_cmd.target_location = self.extract_location(params[1])\n                    elif action_type == 'find':\n                        parsed_cmd.target_object = self.extract_object(params[0])\n\n                    return parsed_cmd\n\n        return None  # Command not recognized\n\n    def extract_location(self, location_text):\n        \"\"\"Extract location from text\"\"\"\n        # Check for predefined locations\n        for loc_name, coords in self.location_keywords.items():\n            if loc_name in location_text:\n                pose = Pose()\n                pose.position.x = float(coords[0])\n                pose.position.y = float(coords[1])\n                pose.position.z = float(coords[2])\n                return pose\n\n        # If not found, return default position\n        pose = Pose()\n        pose.position.x = 0.0\n        pose.position.y = 0.0\n        pose.position.z = 0.0\n        return pose\n\n    def extract_object(self, object_text):\n        \"\"\"Extract object from text\"\"\"\n        # Find the most likely object\n        for obj in self.object_keywords:\n            if obj in object_text:\n                return obj\n\n        # Return the original text if no known object found\n        return object_text\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = CommandParserNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Create a simple message definition for parsed commands (in ",(0,s.jsx)(n.code,{children:"msg/ParsedCommand.msg"}),"):"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"string action\nstring raw_command\nstring target_object\ngeometry_msgs/Pose target_location\nstring[] parameters\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"exercise-3-creating-a-task-decomposition-system",children:"Exercise 3: Creating a Task Decomposition System"}),"\n",(0,s.jsx)(n.p,{children:"Now you'll implement a task decomposition system that breaks complex commands into executable steps."}),"\n",(0,s.jsx)(n.h3,{id:"steps-2",children:"Steps"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Create the task decomposition node ",(0,s.jsx)(n.code,{children:"task_decomposer_node.py"}),":","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom vla_interfaces.msg import ParsedCommand, TaskPlan\nfrom geometry_msgs.msg import Pose\nfrom std_msgs.msg import String\n\n\nclass TaskDecomposerNode(Node):\n\n    def __init__(self):\n        super().__init__('task_decomposer_node')\n\n        # Publishers and Subscribers\n        self.parsed_sub = self.create_subscription(\n            ParsedCommand, 'parsed_commands', self.parsed_callback, 10)\n        self.plan_pub = self.create_publisher(TaskPlan, 'task_plans', 10)\n\n        # Define task templates\n        self.task_templates = {\n            'move_and_grasp': [\n                {'action': 'navigate_to', 'required_params': ['target_location']},\n                {'action': 'detect_object', 'required_params': ['target_object']},\n                {'action': 'approach_object', 'required_params': ['target_object']},\n                {'action': 'grasp_object', 'required_params': ['target_object']}\n            ],\n            'grasp_and_place': [\n                {'action': 'navigate_to', 'required_params': ['start_location']},\n                {'action': 'detect_object', 'required_params': ['target_object']},\n                {'action': 'grasp_object', 'required_params': ['target_object']},\n                {'action': 'navigate_to', 'required_params': ['target_location']},\n                {'action': 'place_object', 'required_params': ['target_object']}\n            ],\n            'find_and_report': [\n                {'action': 'detect_object', 'required_params': ['target_object']},\n                {'action': 'report_location', 'required_params': ['target_object']}\n            ]\n        }\n\n        self.get_logger().info('Task decomposer node initialized')\n\n    def parsed_callback(self, msg):\n        \"\"\"Process parsed commands and generate task plans\"\"\"\n        self.get_logger().info(f'Decomposing command: {msg.action}')\n\n        # Generate task plan based on command type\n        task_plan = self.generate_task_plan(msg)\n\n        if task_plan and len(task_plan.tasks) > 0:\n            self.plan_pub.publish(task_plan)\n            self.get_logger().info(f'Generated plan with {len(task_plan.tasks)} tasks')\n        else:\n            self.get_logger().warn('Could not generate task plan')\n\n    def generate_task_plan(self, parsed_cmd):\n        \"\"\"Generate a task plan from parsed command\"\"\"\n        plan = TaskPlan()\n        plan.header.stamp = self.get_clock().now().to_msg()\n        plan.header.frame_id = 'map'\n        plan.original_command = parsed_cmd.raw_command\n\n        # Determine task template based on action and parameters\n        if parsed_cmd.action == 'grasp' and parsed_cmd.target_location.position.x != 0.0:\n            # This is likely a grasp and place command\n            template = self.task_templates['grasp_and_place']\n\n            # Update parameters for the plan\n            for task in template:\n                if 'start_location' in task['required_params']:\n                    # For now, assume current location is [0,0,0]\n                    task['params'] = {'location': [0.0, 0.0, 0.0]}\n                elif 'target_object' in task['required_params']:\n                    task['params'] = {'object': parsed_cmd.target_object}\n                elif 'target_location' in task['required_params']:\n                    task['params'] = {\n                        'location': [\n                            parsed_cmd.target_location.position.x,\n                            parsed_cmd.target_location.position.y,\n                            parsed_cmd.target_location.position.z\n                        ]\n                    }\n                else:\n                    task['params'] = {}\n\n                # Add to plan\n                plan.tasks.append(self.create_task_msg(task))\n        elif parsed_cmd.action == 'grasp':\n            # Simple grasp command\n            template = [\n                {'action': 'navigate_to', 'required_params': ['target_object'], 'params': {}},\n                {'action': 'detect_object', 'required_params': ['target_object'], 'params': {'object': parsed_cmd.target_object}},\n                {'action': 'grasp_object', 'required_params': ['target_object'], 'params': {'object': parsed_cmd.target_object}}\n            ]\n\n            for task in template:\n                if 'target_object' in task['required_params']:\n                    task['params'] = {'object': parsed_cmd.target_object}\n                plan.tasks.append(self.create_task_msg(task))\n        elif parsed_cmd.action == 'move':\n            # Simple navigation command\n            template = [\n                {'action': 'navigate_to', 'required_params': ['target_location'], 'params': {\n                    'location': [\n                        parsed_cmd.target_location.position.x,\n                        parsed_cmd.target_location.position.y,\n                        parsed_cmd.target_location.position.z\n                    ]\n                }}\n            ]\n\n            for task in template:\n                plan.tasks.append(self.create_task_msg(task))\n        else:\n            # Default: simple action\n            template = [{'action': parsed_cmd.action, 'required_params': [], 'params': {}}]\n            for task in template:\n                plan.tasks.append(self.create_task_msg(task))\n\n        return plan\n\n    def create_task_msg(self, task_dict):\n        \"\"\"Create a Task message from dictionary\"\"\"\n        from vla_interfaces.msg import Task  # Assuming you have this message type\n\n        task_msg = Task()\n        task_msg.action = task_dict['action']\n        task_msg.parameters = json.dumps(task_dict.get('params', {}))\n        task_msg.priority = 0  # Default priority\n\n        return task_msg\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = TaskDecomposerNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"exercise-4-building-language-to-ros-action-mapping-system",children:"Exercise 4: Building Language-to-ROS Action Mapping System"}),"\n",(0,s.jsx)(n.p,{children:"Now you'll create a system that maps language-based task plans to ROS 2 actions and services."}),"\n",(0,s.jsx)(n.h3,{id:"steps-3",children:"Steps"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Create the action mapper node ",(0,s.jsx)(n.code,{children:"action_mapper_node.py"}),":","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom vla_interfaces.msg import TaskPlan, ExecutableAction\nfrom geometry_msgs.msg import Pose\nfrom nav2_msgs.action import NavigateToPose\nfrom rclpy.action import ActionClient\nimport json\n\n\nclass ActionMapperNode(Node):\n\n    def __init__(self):\n        super().__init__('action_mapper_node')\n\n        # Publishers and Subscribers\n        self.plan_sub = self.create_subscription(\n            TaskPlan, 'task_plans', self.plan_callback, 10)\n        self.action_pub = self.create_publisher(\n            ExecutableAction, 'executable_actions', 10)\n\n        # Action clients\n        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')\n\n        # Define action mappings\n        self.action_mappings = {\n            'navigate_to': self.map_navigate_to,\n            'grasp_object': self.map_grasp_object,\n            'place_object': self.map_place_object,\n            'detect_object': self.map_detect_object,\n            'approach_object': self.map_approach_object,\n            'report_location': self.map_report_location\n        }\n\n        self.get_logger().info('Action mapper node initialized')\n\n    def plan_callback(self, msg):\n        \"\"\"Process task plans and map to executable actions\"\"\"\n        self.get_logger().info(f'Processing plan with {len(msg.tasks)} tasks')\n\n        for task in msg.tasks:\n            self.get_logger().info(f'Mapping task: {task.action}')\n\n            # Map the task to an executable action\n            executable_action = self.map_task_to_action(task, msg.header.frame_id)\n\n            if executable_action:\n                self.action_pub.publish(executable_action)\n                self.get_logger().info(f'Published action: {executable_action.action_type}')\n            else:\n                self.get_logger().warn(f'Could not map task: {task.action}')\n\n    def map_task_to_action(self, task, frame_id):\n        \"\"\"Map a task to an executable ROS action\"\"\"\n        if task.action in self.action_mappings:\n            return self.action_mappings[task.action](task, frame_id)\n        else:\n            self.get_logger().warn(f'Unknown action: {task.action}')\n            return None\n\n    def map_navigate_to(self, task, frame_id):\n        \"\"\"Map navigate_to task to ROS action\"\"\"\n        from vla_interfaces.msg import ExecutableAction\n\n        action_msg = ExecutableAction()\n        action_msg.action_type = 'navigate_to_pose'\n        action_msg.header.stamp = self.get_clock().now().to_msg()\n        action_msg.header.frame_id = frame_id\n\n        # Parse parameters\n        params = json.loads(task.parameters) if task.parameters else {}\n\n        if 'location' in params:\n            location = params['location']\n\n            # Create NavigateToPose goal\n            goal = NavigateToPose.Goal()\n            goal.pose.header.frame_id = frame_id\n            goal.pose.pose.position.x = float(location[0])\n            goal.pose.pose.position.y = float(location[1])\n            goal.pose.pose.position.z = float(location[2])\n            # Set orientation to face forward (for simplicity)\n            goal.pose.pose.orientation.w = 1.0\n\n            action_msg.goal = json.dumps({\n                'target_pose': {\n                    'position': location,\n                    'orientation': [0, 0, 0, 1]  # w=1 for no rotation\n                }\n            })\n        else:\n            self.get_logger().warn('Navigate task missing location parameter')\n            return None\n\n        return action_msg\n\n    def map_grasp_object(self, task, frame_id):\n        \"\"\"Map grasp_object task to ROS action\"\"\"\n        from vla_interfaces.msg import ExecutableAction\n\n        action_msg = ExecutableAction()\n        action_msg.action_type = 'grasp_object'\n        action_msg.header.stamp = self.get_clock().now().to_msg()\n        action_msg.header.frame_id = frame_id\n\n        # Parse parameters\n        params = json.loads(task.parameters) if task.parameters else {}\n\n        if 'object' in params:\n            action_msg.goal = json.dumps({'object_name': params['object']})\n        else:\n            self.get_logger().warn('Grasp task missing object parameter')\n            return None\n\n        return action_msg\n\n    def map_place_object(self, task, frame_id):\n        \"\"\"Map place_object task to ROS action\"\"\"\n        from vla_interfaces.msg import ExecutableAction\n\n        action_msg = ExecutableAction()\n        action_msg.action_type = 'place_object'\n        action_msg.header.stamp = self.get_clock().now().to_msg()\n        action_msg.header.frame_id = frame_id\n\n        # Parse parameters\n        params = json.loads(task.parameters) if task.parameters else {}\n\n        if 'object' in params:\n            action_msg.goal = json.dumps({'object_name': params['object']})\n        else:\n            self.get_logger().warn('Place task missing object parameter')\n            return None\n\n        return action_msg\n\n    def map_detect_object(self, task, frame_id):\n        \"\"\"Map detect_object task to ROS action\"\"\"\n        from vla_interfaces.msg import ExecutableAction\n\n        action_msg = ExecutableAction()\n        action_msg.action_type = 'detect_object'\n        action_msg.header.stamp = self.get_clock().now().to_msg()\n        action_msg.header.frame_id = frame_id\n\n        # Parse parameters\n        params = json.loads(task.parameters) if task.parameters else {}\n\n        if 'object' in params:\n            action_msg.goal = json.dumps({'object_name': params['object']})\n        else:\n            self.get_logger().warn('Detect task missing object parameter')\n            return None\n\n        return action_msg\n\n    def map_approach_object(self, task, frame_id):\n        \"\"\"Map approach_object task to ROS action\"\"\"\n        from vla_interfaces.msg import ExecutableAction\n\n        action_msg = ExecutableAction()\n        action_msg.action_type = 'approach_object'\n        action_msg.header.stamp = self.get_clock().now().to_msg()\n        action_msg.header.frame_id = frame_id\n\n        # Parse parameters\n        params = json.loads(task.parameters) if task.parameters else {}\n\n        if 'object' in params:\n            action_msg.goal = json.dumps({'object_name': params['object']})\n        else:\n            self.get_logger().warn('Approach task missing object parameter')\n            return None\n\n        return action_msg\n\n    def map_report_location(self, task, frame_id):\n        \"\"\"Map report_location task to ROS action\"\"\"\n        from vla_interfaces.msg import ExecutableAction\n\n        action_msg = ExecutableAction()\n        action_msg.action_type = 'report_location'\n        action_msg.header.stamp = self.get_clock().now().to_msg()\n        action_msg.header.frame_id = frame_id\n\n        # Parse parameters\n        params = json.loads(task.parameters) if task.parameters else {}\n\n        if 'object' in params:\n            action_msg.goal = json.dumps({'object_name': params['object']})\n        else:\n            self.get_logger().warn('Report task missing object parameter')\n            return None\n\n        return action_msg\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ActionMapperNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"exercise-5-complete-vla-integration",children:"Exercise 5: Complete VLA Integration"}),"\n",(0,s.jsx)(n.p,{children:"Finally, you'll create a launch file that brings together all the VLA components."}),"\n",(0,s.jsx)(n.h3,{id:"steps-4",children:"Steps"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Create a launch file ",(0,s.jsx)(n.code,{children:"vla_system_launch.py"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import os\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, RegisterEventHandler\nfrom launch.event_handlers import OnProcessStart\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\n\n\ndef generate_launch_description():\n    # Declare launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time')\n\n    # Voice ingestion node\n    voice_ingestion_node = Node(\n        package='vla_voice_ingestion',\n        executable='voice_ingestion_node.py',\n        name='voice_ingestion_node',\n        parameters=[{'use_sim_time': use_sim_time}],\n        output='screen'\n    )\n\n    # Command parser node\n    command_parser_node = Node(\n        package='vla_command_parsing',\n        executable='command_parser_node.py',\n        name='command_parser_node',\n        parameters=[{'use_sim_time': use_sim_time}],\n        output='screen'\n    )\n\n    # Task decomposer node\n    task_decomposer_node = Node(\n        package='vla_task_decomposition',\n        executable='task_decomposer_node.py',\n        name='task_decomposer_node',\n        parameters=[{'use_sim_time': use_sim_time}],\n        output='screen'\n    )\n\n    # Action mapper node\n    action_mapper_node = Node(\n        package='vla_action_mapping',\n        executable='action_mapper_node.py',\n        name='action_mapper_node',\n        parameters=[{'use_sim_time': use_sim_time}],\n        output='screen'\n    )\n\n    # Action executor node (placeholder - you would implement this)\n    action_executor_node = Node(\n        package='vla_action_execution',\n        executable='action_executor_node.py',\n        name='action_executor_node',\n        parameters=[{'use_sim_time': use_sim_time}],\n        output='screen'\n    )\n\n    return LaunchDescription([\n        voice_ingestion_node,\n        command_parser_node,\n        task_decomposer_node,\n        action_mapper_node,\n        action_executor_node\n    ])\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Launch the complete VLA system:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 launch vla_system_launch.py\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Test the system by speaking commands to your microphone and observing the processing pipeline."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Voice command ingestion requires real-time audio processing and ASR systems"}),"\n",(0,s.jsx)(n.li,{children:"Natural language parsing converts text commands to structured representations"}),"\n",(0,s.jsx)(n.li,{children:"Task decomposition breaks complex commands into executable steps"}),"\n",(0,s.jsx)(n.li,{children:"Action mapping connects language-based tasks to ROS 2 actions and services"}),"\n",(0,s.jsx)(n.li,{children:"The complete VLA system integrates all components for natural human-robot interaction"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Automatic Speech Recognition (ASR)"}),": Converting speech to text"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Natural Language Understanding (NLU)"}),": Interpreting the meaning of text"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Task Decomposition"}),": Breaking high-level commands into primitive actions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action Mapping"}),": Connecting language concepts to robot capabilities"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time Processing"}),": Handling voice input with low latency"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Command Parsing"}),": Extracting intent and entities from natural language"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Executable Actions"}),": ROS 2 actions and services that implement robot behaviors"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"practical-exercises",children:"Practical Exercises"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Implement a VLA system for your specific robot platform"}),"\n",(0,s.jsx)(n.li,{children:"Add support for more complex commands and interactions"}),"\n",(0,s.jsx)(n.li,{children:"Integrate visual feedback with voice commands"}),"\n",(0,s.jsx)(n.li,{children:"Implement error recovery and human-in-the-loop corrections"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"common-failure-modes",children:"Common Failure Modes"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Audio Quality Issues"}),": Background noise affecting speech recognition accuracy"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Command Ambiguity"}),": Natural language commands not clearly resolved to actions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Timing Issues"}),": Delays in processing affecting natural interaction"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Context Confusion"}),": System failing to maintain context across command sequences"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Resource Limitations"}),": Computational constraints affecting real-time performance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Misinterpretation"}),": Language commands mapped to incorrect robot actions"]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(m,{...e})}):m(e)}},8453(e,n,a){a.d(n,{R:()=>i,x:()=>r});var t=a(6540);const s={},o=t.createContext(s);function i(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);